# Best Practices and Antipatterns for PostgreSQL (with Prisma) and Redis in a Microservices Backend

## Security

### PostgreSQL (with Prisma ORM)

- **Best Practices:**
- **Principle of least privilege:** Create dedicated database roles/users for each microservice with only the necessary privileges. Avoid using the superuser (e.g. postgres or AWS rds_superuser) for application access. Instead, grant minimal rights via roles (e.g. a role that cannot CONNECT by default, and then grant it specific table or schema privileges). Separate roles for read-only vs read-write as needed.
- **Network access control:** Restrict database access to only the networks or services that need it. In AWS, use security groups and NACLs to allow inbound DB traffic only from application servers, not the open internet. On bare metal or VMs, bind Postgres to a private interface and use firewall rules to block unwanted access.
- **Encrypt data in transit and at rest:** Enable SSL/TLS for PostgreSQL connections and enforce it (for example, set rds.force_ssl=1 on AWS RDS). This ensures even within a VPC, data is encrypted on the wire. Encrypt data at rest using disk encryption or cloud KMS (AWS RDS can encrypt storage with KMS).
- **Secure credentials management:** Store database credentials in secure secret stores (AWS Secrets Manager, Vault, etc.) rather than in code or config files. Rotate passwords regularly. In AWS, you can use IAM authentication for RDS PostgreSQL to avoid static passwords.
- **Role and schema isolation:** Give each microservice its own schema (or even its own database) to namespace its tables. Avoid using the default public schema for application tables, because all users can access it by default. By isolating schemas and using role-based access, you prevent one service from accidentally touching another's data.
- **Use ORMs to prevent injection - but remain vigilant:** Prisma ORM by default uses parameterized queries, which guards against SQL injection. Still, validate inputs on the application side (especially if using raw SQL through Prisma). Never construct raw queries by string concatenation with untrusted input.
- **Auditing and logging:** Enable database auditing for security-sensitive data. PostgreSQL's log_statement can log queries; for deeper audit, consider the pgAudit extension. Monitor failed login attempts or suspicious queries. However, be cautious not to log sensitive data (e.g. don't log full statements that include passwords or personal data) - Postgres can log those in clear text if not configured carefully.
- **Patching and updates:** Keep PostgreSQL up to date with security patches. Update Prisma ORM to latest versions as well, since the ORM could have its own security fixes. In AWS, use automated minor version upgrades if possible for RDS.
- **Antipatterns:**
- **Using superuser accounts in applications:** For example, connecting with the postgres role for convenience. This can lead to accidental destructive commands. It's safer to use a limited user and elevate privileges only when needed for migrations or admin tasks.
- **Exposing the database to the internet:** Running Postgres on a public IP or broad network without restriction is dangerous. This can lead to brute-force attacks or data exposure. Always prefer private networks or VPN access.
- **No encryption:** Allowing unencrypted connections or storing sensitive data without at-rest encryption. This risks data interception and violates compliance. Enforce SSL (e.g. require sslmode=verify-full in the connection string and use certificates).
- **Hard-coding secrets:** Embedding database passwords or keys in the application code or container images. This can leak via source control. Instead, use environment variables provided at runtime or secret management services.
- **Using the default public schema for everything:** This is a default that grants broad access. If any compromised account has minimal access, they could potentially read from public schema tables. Not organizing schemas also makes migrations and permissioning harder in multi-tenant or multi-service DBs.
- **Ignoring user activity logging:** Failing to log or monitor DB activities - you may miss early signs of breaches. At minimum, log failed logins and consider tools like CloudTrail (for AWS RDS API actions) and CloudWatch logs or pgAudit for detailed query auditing.
- **Lax database user management:** e.g., using one DB user for all services or developers. This prevents accountability and least privilege. It's an antipattern to share accounts or to reuse the application account for ad-hoc queries or admin tasks.

### Redis

- **Best Practices:**
- **Enable authentication and authorization:** Use Redis ACLs (Access Control Lists) introduced in Redis 6+ to create users with only needed commands (e.g., disable dangerous commands for untrusted users). Always require a strong requirepass (or ACL user password) for Redis, even inside a private network, to prevent unauthorized access.
- **Network security for Redis:** Never expose Redis directly on the internet. Bind it to 127.0.0.1 or an internal network interface. Use firewall rules or cloud security groups to only allow access from your application servers. In cloud setups (AWS Elasticache), by default it's in a VPC - still ensure security groups are strict.
- **Use TLS for Redis if available:** Use Redis with TLS enabled for encryption in transit (AWS Elasticache, Azure Cache, and Redis Enterprise support TLS). This protects data like session tokens or sensitive cache values from eavesdropping. (Azure, for instance, requires TLS by default).
- **Regularly update and patch:** Stay on supported versions of Redis and apply updates to get security fixes. Similarly, update Redis client libraries in your microservices to pick up any security patches (e.g., vulnerabilities in older redis npm packages).
- **Secure configurations:** Disable or rename dangerous commands that are not needed in production. For example, FLUSHALL, DEBUG, CONFIG and SHUTDOWN can be renamed to prevent accidental or malicious use. Also, keep Redis in "protected mode" (which binds to localhost) unless you intentionally configure it otherwise.
- **Isolate by instance when needed:** If you have multi-tenant data or mixed-security contexts, use separate Redis instances rather than Redis logical databases for strong isolation. The use of the SELECT command for multiple logical DBs in one instance is generally discouraged in microservices - it can lead to security and maintenance issues.
- **Monitor and restrict access keys:** On AWS, use IAM roles/policies for Elasticache if integrating with other services (though Elasticache doesn't natively use IAM for Redis, you control network access). For self-managed, consider tools like stunnel or VPN tunnels for an extra layer when crossing untrusted networks.
- **Antipatterns:**
- **Running Redis without a password (AUTH):** A default Redis installation has no password and will accept connections from anywhere if not firewalled. This is extremely unsafe - there have been worms exploiting open Redis instances. Always require AUTH in production.
- **Allowing broad network access:** E.g., binding Redis to 0.0.0.0 and not restricting ports. This could allow any other service or even external actors to read/write your cache or queue. Redis has no per-key security model, so a connected client can typically do anything unless limited by ACLs.
- **No encryption on sensitive data:** If you put user session data or PII in Redis, not using TLS or encryption means that anyone sniffing the traffic can steal that data. Also, not encrypting backups or RDB/AOF files (if they contain sensitive info) is a risk if those files are exposed.
- **Using default settings in production:** For instance, leaving protected-mode off on a server that might accidentally be in a less secure network, or not adjusting the maxclients (which can lead to Redis crashes if too many connections).
- **Credential mismanagement:** Hardcoding the Redis password in code or config where it could leak (similar to DB credentials). Or reusing the same password across environments - which could lead to a dev environment breach affecting prod if network rules are not strict.
- **Overlooking ACLs for untrusted scenarios:** If you provide any multi-tenant access to a Redis (rare in microservices, but possibly for a shared cache service), not using ACL rules to limit commands (e.g. disabling keys listing, config change) can lead to abuse.
- **No plan for key management:** If you rotate secrets regularly for compliance, failing to plan for Redis password rotation (and updating apps accordingly) is an antipattern. This can lead to either stale credentials causing outages or never rotating credentials at all.

## Performance and Scalability

### PostgreSQL (with Prisma)

- **Best Practices:**
- **Use indexing and query optimization:** Ensure your PostgreSQL tables have proper indexes for the queries your microservices run. Monitor slow queries (e.g., via pg_stat_statements) and add indexes or rewrite queries as needed. Prisma allows writing raw queries or using the query builder; use those to optimize hotspots when the generated queries aren't optimal.
- **Avoid N+1 query patterns:** In microservices, if a service needs to fetch related data, avoid running a query in a loop. Use Prisma's relational querying (e.g., include or select with relations) to retrieve data in a single query when possible. If Prisma's API is limited, consider raw SQL with JOINs. N+1 queries can significantly degrade performance as data volume grows.
- **Connection pooling:** Reuse database connections to reduce overhead. Every Prisma Client maintains an internal pool of connections - configure its size appropriately (Prisma uses Node-Postgres under the hood). For high load, ensure the pool is large enough to utilize the DB, but not so large that Postgres is overwhelmed by context switching. If you have _many_ microservice instances, consider a proxy like PgBouncer or AWS RDS Proxy to efficiently pool at scale.
- **Scale read operations with replicas:** If certain services are read-heavy (e.g., an analytics service), leverage read replicas. Configure the service (or use a secondary Prisma client instance) to connect to replicas for read queries. This offloads work from the primary. Ensure the application can tolerate the slight replication lag for these reads (eventual consistency for replicas).
- **Optimize writes and transactions:** Batch writes when possible (Prisma supports createMany and transactional inserts). For example, inserting 100 rows in one transaction is much faster than 100 individual inserts. Use transactions for grouped updates to reduce round-trip overhead and ensure consistency.
- **Leverage caching for expensive reads:** For data that is frequently read but not frequently changing, use an in-memory cache or Redis in front of Postgres (cache-aside). This can take load off the database and improve latency. Just ensure cache invalidation is handled (discussed later).
- **Right-size your database instances:** Monitor CPU, memory, and I/O on your Postgres. For AWS RDS, choose instance classes with enough memory to hold your working set in RAM (to avoid excessive disk reads). Use shared_buffers and other parameters appropriately (cloud managed DBs auto-tune many of these). Scaling vertically (bigger instance) can give immediate performance gains, but also plan for horizontal scaling if needed (like partitioning data by tenant, or sharding by some key).
- **Use efficient data models:** Normalize where appropriate to avoid data anomalies, but also consider denormalization for extremely performance-sensitive reads (at the cost of more complex writes). PostgreSQL JSONB can be useful for flexible schemas, but don't misuse it for data that is better relationally indexed. If using JSONB, index keys you query by.
- **Monitoring and tuning:** Continuously monitor query performance and utilize Postgres performance features. For instance, enable pg_stat_statements to find the most frequent and slowest queries. Tune settings like work_mem for large sort or hash operations if a microservice does heavy reporting queries.
- **Antipatterns:**
- **One database for all microservices:** This creates a monolithic bottleneck and couples services at the data level. It can become a performance nightmare as unrelated services all compete for the same resources. The recommended practice is database-per-service to localize performance impacts and schema changes.
- **Excessive open connections:** Each microservice instance with a large connection pool (e.g., default 10 connections) can overwhelm Postgres if you have many instances. Postgres has a connection limit and performance degrades with too many active connections. It's an antipattern to think "more connections = more throughput" beyond a point. Use pooling and tune the pool size based on load tests. For serverless environments, not using a proxy (so every function invocation opens new connections) is a known anti-pattern - it can exhaust the DB quickly.
- **Lack of indexes or bad indexing:** Not indexing a frequently filtered column will cause sequential scans and high CPU I/O usage. Conversely, having too many indexes can slow writes. An anti-pattern is to copy a schema to Postgres without considering indexes (e.g., relying on Prisma to manage data but not manually reviewing if indexes are needed beyond primary keys). Always review query plans for slow queries.
- **Select \* from everything:** Pulling more data than needed. This wastes network and memory. In microservices, sending huge result sets over the network (especially between regions or zones) can be a bottleneck. Only select the fields you need and paginate results. Also, avoid transferring large BLOBs or CLOBs repeatedly - consider storing large objects in S3 or a blob store and only store references in Postgres.
- **Ignoring query performance in ORMs:** Assuming Prisma or any ORM will automatically handle performance. ORMs can produce suboptimal SQL. It's a mistake to never inspect the generated queries. For example, including too many relations in one query might join many tables and cause a huge result set or heavy join operation. Not understanding these can lead to slow responses and DB timeouts.
- **Overuse of transactions or locking:** Holding locks for long periods by doing large updates or slow computations in a transaction can throttle concurrent operations. For instance, an anti-pattern is wrapping an entire complex business process in one DB transaction unnecessarily - it reduces concurrency and can lead to lock waits. Use transactions minimally for the necessary atomic sections.
- **No caching or optimization at the app layer:** Hammering the database for every read, even for data that rarely changes, is inefficient. Microservices that don't implement any caching (local in-memory or distributed) and hit Postgres for each request will struggle to scale.
- **Ignoring platform-specific performance features:** E.g., on AWS, not using provisions like **Aurora** (which can give better read scaling and auto-tuning) for high scale, or not using performance insights. On bare metal, failing to configure parameters (leaving defaults like 8MB shared_buffers which is terribly low) is an anti-pattern.
- **Premature sharding of relational data:** Sharding Postgres at the application level too early can add a lot of complexity (distributed transactions, etc.). Until you truly exceed what a single instance (or a primary-replica setup) can handle, scale up or use read replicas. Over-sharding can actually hurt performance if each shard becomes underutilized but still has overhead.

### Redis

- **Best Practices:**
- **Use the right data structures:** Redis offers strings, hashes, lists, sets, sorted sets, streams, etc. Pick structures that model your data efficiently. For example, use hashes to store related attributes under one key (to fetch them in one call), use sorted sets for leaderboards, use sets for memberships, etc. Efficient use of data types reduces the need for multiple calls and memory overhead.
- **Keep values reasonably sized:** Redis operations are usually O(size of data touched). Very large values (multi-MB blobs) will degrade performance during GET/SET. It's often better to split large data into smaller chunks or store large blobs outside Redis (and keep just references). Azure's guidance notes Redis works best with many small values rather than a few huge values. As a rule of thumb, values in the few kilobytes range are fine; if you have 100KB+ values, consider whether that's optimal.
- **Leverage pipelining for throughput:** If a microservice needs to perform many Redis operations, use pipelining (or multi/exec transactions if atomicity is needed for multiple keys in one shard). Pipelining sends multiple commands in one network round-trip. This dramatically improves throughput by reducing latency impact. Many Redis client libraries support this natively.
- **Use Redis Cluster or sharding for scale:** A single Redis instance is single-threaded for command processing. If you have heavy workloads (e.g., > 50k ops/sec or datasets > 20-30 GB), plan to shard your data. Redis Cluster mode allows horizontal scaling to multiple shards. As a rule of thumb, consider sharding if you exceed ~25 GB or ~25k ops/sec on one node. Cluster mode will also give you more memory headroom and CPU across nodes.
- **Reuse connections and limit connection count:** Creating new TCP connections for each operation is expensive. Use persistent connections or a connection pool. Many clients (like redis-py, Node Redis) use connection pooling by default. If you have many microservices, be mindful of total connections - thousands of clients can overwhelm Redis (which handles ~65k connections). In scenarios with massive fan-out, use a proxy like Twemproxy or Redis Cluster Proxy to reduce direct connections. Long-lived connections also allow you to use Pub/Sub efficiently and avoid reconnect overhead.
- **Read from replicas if appropriate:** For read-heavy scenarios that can tolerate slightly stale data, enable a replica and offload read operations to it (Redis replicas are async by default and can serve reads). In AWS Elasticache, you can have reader endpoints or even auto-read from the replica in the same AZ for lower latency. This is mostly useful when using Redis as a primary data store or large cache where one node's CPU is bottlenecked by reads.
- **Avoid expensive commands and patterns:** Certain Redis commands are known to be O(N) or worse (e.g., KEYS, SCAN can be O(N) over the keyspace; SMEMBERS returns entire set which can be huge). In production, **never use KEYS \* on a large instance**. Use SCAN for iterative traversal if needed, and design key naming conventions so you don't need to scan frequently. Similarly, avoid operations that block the server for long time (like huge SORT operations or long Lua scripts).
- **Use expiration (TTL) wisely:** For cache use-cases, always set an appropriate TTL on keys so that they eventually evict. This prevents unlimited memory growth and naturally clears out old data. Even for non-cache data, if it's not needed after a point (like session tokens), use expirations. Use policies like Least Recently Used (LRU) or Least Frequently Used (LFU) for eviction on caches (both are available in Redis). A common strategy is to set a TTL plus some jitter to avoid all keys expiring at once.
- **Scale horizontally by function if needed:** If you find one Redis instance is being hot for a particular type of data (say, user sessions), you can split data domains across different Redis instances. For example, one for sessions, one for caching product data, one for queues. This can be easier than sharding when the use cases are distinct, and it isolates workloads (so a surge in one doesn't slow the others).
- **Lua scripting best practices:** If you use Lua (EVAL scripts) for atomic operations, keep scripts short and efficient. Declare all keys used in the script up front (which is required for Redis cluster to route to the right shard). Do not run long loops in Lua - it blocks Redis server during execution. If a script might take too long, consider breaking it into smaller pieces or moving that logic to the application if possible.
- **Pub/Sub and streaming optimizations:** For high-throughput Pub/Sub on cluster, use Redis 7's _sharded pub/sub_ to avoid broadcasting to all shards. Essentially, design your channels with a shard prefix (or let Redis 7 handle it automatically) so that messages go only to subscribers on the relevant shard, reducing overhead.
- **Antipatterns:**
- **Huge single instance with no sharding:** Running a single Redis node with an enormous dataset (say 100GB of data) and extremely high throughput can lead to long pauses during RDB snapshots, slow failovers, and increased risk of loss on failure. An anti-pattern is pushing one node to its limits instead of adding more nodes. For both availability and performance, multiple smaller nodes (with clustering) is better than one giant node.
- **Opening and closing connections per operation:** For example, if your microservice connects to Redis, does one GET/SET, then disconnects, on every request - this adds TCP overhead and can overwhelm Redis with connection handling. This was shown to massively increase latency (on the order of 10x slower) compared to reused connections. Always reuse connections or use pooling. A reconnect flood after a network blip is also dangerous - if many clients reconnect simultaneously, it can spike load and even cause failover.
- **No key expirations on cache data:** Caching without TTL or eviction means your cache can grow indefinitely and also serve stale data. It's an anti-pattern to use Redis as an _infinite_ cache. Forgetting to set expirations will eventually fill up memory and trigger evictions of some key (if an eviction policy is set) or cause out-of-memory errors (if noeviction policy). This can be catastrophic at runtime. Always decide a caching strategy (TTL or explicit invalidation).
- **"Hot key" overload:** Having one or a few keys that receive a disproportionate amount of traffic (e.g., a single key that all requests update or read) can bottleneck Redis (since operations are single-threaded per key). This anti-pattern often happens if an improperly sharded counter or a single global lock is stored in Redis. Mitigate by sharding the key (e.g., instead of one global counter key, use N keys and sum them, or otherwise distribute writes). Also monitor for hot keys using Redis's --hotkeys option.
- **Using Redis as a SQL database:** Trying to simulate complex queries by scanning keys or using Redis as a primary store without adjusting design. For example, storing data in a way that you frequently need to SCAN the entire keyspace (similar to a full table scan) is a bad practice - it will be slow and block the server. If you find yourself needing to query Redis by value, consider using Redis Search module or re-evaluate if that data belongs in a database optimized for queries.
- **Blocking on long operations:** Certain patterns like very large LRANGE on a huge list or iterating a big sorted set with thousands of members in one call can block Redis. It's an antipattern to treat Redis as if it can handle unlimited data per command. If you need to retrieve large datasets, do it in chunks (e.g., use cursors with SCAN/SSCAN or range queries in smaller windows).
- **Ignoring memory limits and eviction strategy:** Not configuring maxmemory at all and assuming the OS will handle it can lead to the OS swapping or OOM-killing Redis. On the other hand, setting maxmemory but choosing the wrong eviction (or leaving default volatile-lru which evicts only keys with TTL) can cause issues. For primary data, eviction should often be noeviction (so you prefer an error to losing data). For caches, use an eviction policy like allkeys-lru or allkeys-lfu. An anti-pattern is not aligning this with your use-case and thus either crashing or evicting critical data.
- **Over-subscribing to Pub/Sub channels:** In Pub/Sub, if you have thousands of subscribers on a channel but only a few actually needed, you waste resources. Also, a slow subscriber can slow down the publisher (Redis queues messages per subscriber). It's an anti-pattern to have very slow consumers or more subscribers than necessary on hot channels. If a subscriber can't keep up, Redis will start queuing messages to it and could ultimately drop the connection to preserve memory. Design your publish/subscribe usage so that each service only subscribes to relevant channels, and consider message size limits.
- **Misuse of Lua scripts in cluster:** Using a Lua script that accesses keys across different hash slots will fail in cluster mode (Redis will block it) unless you use the SCRIPT with KEYS consistently. An anti-pattern is not specifying keys in EVAL or trying to update keys on different shards in one script. The best practice is to operate on keys from one slot or use redis.call("MOVED") responses properly (which client libraries handle). If multi-key atomic ops across shards are needed, maybe Redis isn't the right tool for that part of the system.
- **Not scaling client-side with Redis scaling:** If you add Redis shards or replicas, but your client configuration is static and doesn't utilize them, that's an issue. For example, adding replicas but not using read from replicas, or clustering but using a non-cluster-aware client - you won't get the benefit. Always ensure your clients know about new nodes (cluster clients auto-discover cluster topology, but if you manually shard, update your hashing configuration).

## Resilience, Durability, and Recovery

### PostgreSQL (with Prisma)

- **Best Practices:**
- **Database-per-service isolation for resilience:** In a microservice architecture, each service having its own database means that if one database goes down, it only impacts that service. This limits blast radius. It also simplifies recovery - you're dealing with smaller, service-scoped databases.
- **High Availability setup:** Use replication and automatic failover to avoid downtime. On AWS, enable Multi-AZ for RDS or use Amazon Aurora (which replicates 6 ways by default). This ensures that if the primary fails, a standby is promoted quickly. On bare metal or self-managed, consider solutions like Patroni or repmgr with a quorum mechanism to auto-promote a replica.
- **Regular backups and point-in-time recovery (PITR):** Enable automated backups (for example, RDS can do daily snapshots and PITR up to X days). Also periodically test restoring those backups to verify they work. For self-hosted, archive WAL segments to a secure remote store (like S3 or a backup server) so you can perform PITR if needed. Design a backup retention policy that meets business RPO (Recovery Point Objective).
- **Durability settings:** PostgreSQL is durable by default (WAL and fsync). Ensure settings like synchronous_commit are appropriately tuned. In most cases, synchronous_commit=on (the default) is fine (each commit waits for WAL flush to disk). If you have a replica and can afford minimal data loss, you might use synchronous_commit=local (which doesn't wait for replica ACK). But avoid turning off sync commit in production unless you understand the risk (could lose recent transactions on crash).
- **Graceful application recovery:** Design microservices to handle DB outages gracefully. For example, implement retry logic with exponential backoff for transient database connection failures. If Prisma throws an error like P1001 (connection timeout) or similar during a failover, have your service catch it, log it, and retry after a short delay. This prevents a thundering herd of instant retries that could overwhelm a recovering database.
- **Health checks and circuit breakers:** Use health checks to detect DB connectivity issues. Many microservices will consider themselves unhealthy (and signal to the orchestrator) if they cannot connect to their DB. Additionally, a circuit breaker pattern can prevent your service from overloading a DB that is struggling - e.g., if many queries start failing, the service could temporarily refuse new requests (or serve fallback responses) until the DB recovers.
- **Test failover and recovery processes:** Don't assume failover is instantaneous or that your code will handle it. In staging environments, simulate a database failover (in RDS you can reboot with failover, for example) to see how the service behaves. For backups, simulate a scenario where you must restore the DB from backup and point the service to it - this tests your recovery time and procedure.
- **Use transactions to maintain consistency on failure:** Within a single service's DB operations, encapsulate changes in transactions so that if the service crashes mid-operation, partial updates aren't left in the DB. This is local resilience for consistency. For cross-service workflows, use compensating transactions or a Saga pattern to recover if a multi-step process fails in the middle (since a distributed transaction across services is not available).
- **Design for read-only or degraded mode:** In some failure scenarios (like the primary DB is down but a read replica is still up), it might be useful to allow the service to operate in a read-only mode (serving cached or stale data) rather than being completely down. This might not apply to all services, but consider if your service can have a degraded mode instead of total outage when the DB is unavailable.
- **Antipatterns:**
- **Single point of failure database:** Running Postgres in a single-node configuration with no replicas and relying on "it won't fail." This will eventually lead to an outage (even for maintenance you'll need downtime). In production microservices, this is unacceptable for any critical data.
- **No backups or untested backups:** Not taking backups is an obvious risk. Equally bad is taking backups but never testing restore - you might find the backups were misconfigured or too slow to restore when disaster strikes. An anti-pattern is assuming cloud providers' automated backups don't need verification. You should know how long a restore takes and have run books for it.
- **Ignoring failover impacts:** In a failover scenario, existing connections will break. If the application doesn't handle that (e.g., old connections in Prisma's pool might become invalid), you may get a lot of errors. An anti-pattern is not using a connection check or refresh logic. Modern ORMs often handle this, but be aware that after a failover, you might need to reconnect. If you rely on IP addresses for DB (instead of hostnames), failover will break unless you update IPs - always use the cluster endpoint or DNS that moves on failover.
- **Inadequate resource provisioning for recovery:** For instance, if your backup restore or replication setup is very slow (say your disk I/O or network is a bottleneck), recovery could take too long to meet SLAs. An anti-pattern is to not plan capacity for the worst case. For example, if restoring a 1TB database from S3 takes 4 hours, can your business handle 4 hours downtime? If not, you need solutions like warm standby or faster disk throughput.
- **No monitoring of replication**: Assuming your replicas are healthy without monitoring. Sometimes replication can lag or stop (maybe due to network issues or a bad query on replica). Not monitoring replica lag and health is an anti-pattern; you might fail over to a replica that is hours behind. Always monitor pg_replication_slot lag or CloudWatch's Replica Lag metric and alert if it goes beyond a threshold.
- **Tight coupling of services despite DB-per-service:** If your services depend on each other's database state (without proper isolation), a failure in one DB could cascade. E.g., Service A's DB is down, Service B timeouts waiting for data from A. This is an architectural anti-pattern. Prefer asynchronous communication or fallback logic so that if one service (and its DB) is down, others can still function in a limited capacity.
- **Running without autovacuum or maintenance:** Neglecting Postgres maintenance can lead to bloat and eventually huge performance issues or outages (transaction ID wraparound, etc.). It's an anti-pattern to turn off autovacuum or never monitor it. Ensure vacuum runs, and plan for reindexing or partitioning large tables as needed so that recovery and restart times remain reasonable.
- **Storing state that cannot be regenerated:** If a microservice's DB is caching data from another source (like precomputed data), ensure you can rebuild it if lost. If you treat such a cache as durable but don't back it up, that's a risk. Either back it up or be able to recompute it. For example, losing a read model that can be rebuilt from an event log is fine (just slower performance until rebuilt), but losing it and not having source of truth is bad.

### Redis

- **Best Practices:**
- **Use Redis persistence for primary data:** If you use Redis as a primary database (not just a cache), enable persistence. Redis offers RDB snapshots and Append-Only File (AOF). AOF (with appendfsync set to always or every second) logs every write and can restore the dataset on restart with minimal data loss. Snapshots (RDB) periodically dump the dataset to disk. It's common to use both: AOF for durability and RDB for faster restarts. In managed environments, simply ensure "Persistence" is enabled (e.g., AWS Elasticache can take snapshots).
- **High Availability via replication:** Run at least one replica of each Redis node. If the primary fails, the replica can take over. Use Redis Sentinel or Redis Cluster for automatic failover coordination. Sentinel is suitable for a primary-replica pair (or small set) and will elect a new master if needed, whereas Cluster can failover each shard independently. In AWS, use cluster mode or Redis with Multi-AZ (which sets up a replica in another AZ and auto-failovers). Test these failovers to ensure they're working.
- **Design for cache failure (graceful degradation):** If Redis is used as a cache, your application should tolerate it being unavailable. A common best practice is to implement a fallback to the original data source (like PostgreSQL) when a cache miss or Redis outage occurs, even if it's slower. The system may run in a degraded mode (higher latency) but not completely break. Also consider using a small local cache or circuit breaker to avoid slamming the DB if the cache layer fails.
- **Define clear recovery procedures:** For caches, recovery might just mean warming the cache (which could be done lazily or through a script). For persistent Redis data, have a plan to restore from backups if both primary and replicas are lost. Take backups of Redis (RDB snapshots) and store them off-node (e.g., in S3 or a volume that's regularly snapshotted). This is important for bare-metal setups. In AWS, you can schedule Elasticache snapshots to S3. Verify you can spin up a new Redis from a snapshot and that your microservices can reconnect to it.
- **Active monitoring of Redis health:** Monitor replication health (replicas should be in sync). Monitor for RDB or AOF errors (Redis logs an error if it can't persist to disk). If you see AOF writes are failing or disabled, that's a red flag. Also monitor memory fragmentation and OOM events. Set alerts for keys eviction events - if you see evictions in a primary store (where you expected none), it means you hit maxmemory and data was dropped, which is a serious issue.
- **Sentinel/Cluster configuration:** If self-managing, ensure that Sentinels are deployed in a resilient way (e.g., 3 sentinel instances, possibly on separate hosts or instances, so that they can form a quorum). If using cluster, make sure to deploy an odd number of master shards or handle the majority for quorum (open-source Redis Cluster will not failover if it doesn't have majority). Also, in cluster mode, avoid having all primaries on one rack/zone - spread them to reduce correlated failures.
- **Client retry logic for Redis:** Similar to databases, handle transient connection issues. For example, if a failover happens, clients might see READONLY errors or get connection resets. Implement retry with backoff. Many Redis client libraries will auto-retry or auto-discover the new master (especially with Sentinel or Cluster). Ensure this is enabled and tested. Also consider using a short timeout on operations, so your app isn't hung if Redis is hung (and combine with circuit breakers to stop bombarding a down instance).
- **Durability trade-offs:** If you need **strong durability** in Redis (rare, since it's memory-first), consider enabling AOF with appendfsync=always (every write to disk, but this hits throughput) or every-second (balance between performance and a potential 1s of data loss on crash). Also consider writing to disk synchronously at critical points (Redis has a WAIT command to wait for replication to replicas). These can add latency, so use judiciously on critical writes. The key is to be aware of what data you can afford to lose vs not. For example, cache data - you can lose it. For a queue of jobs - you might not want to lose any, so AOF persistence and replication is important.
- **Thorough testing of failure modes:** Induce failures in a non-prod environment - kill the Redis process or node and see if the replica takes over and if the microservice reconnects. Also test what happens if Redis runs out of memory or if network latency spikes (simulate with tc or a Redis delay). These tests can reveal issues in how the application handles Redis going slow or down.
- **Antipatterns:**
- **Using Redis as a primary store with no HA or persistence:** E.g., running a single Redis instance, in-memory only, for critical data (sessions, shopping carts, job queues) and assuming it never crashes. If it restarts (due to crash or maintenance), all data is gone. This is fine for a pure cache (just a performance issue) but disastrous for a primary store. Unfortunately, this anti-pattern is common when people treat Redis as "just another database" without realizing the volatility. Always configure persistence and replicas for primary data.
- **Not accounting for failover in clients:** Some apps assume a single Redis endpoint and if a failover happens (IP or role change), they don't reconnect properly (for instance, continuing to talk to the old IP which might now be a replica in read-only). This is an anti-pattern - using IPs or static configs instead of proper discovery. Use the cluster client or sentinel support in your Redis library so it auto-discovers the current primary.
- **No backup for Redis data that isn't elsewhere:** If the only place a piece of data lives is Redis (and it's important data), not backing it up is risky. For caches, this doesn't apply; for anything else, ask "if Redis lost all data, can we recover?". If not easily, then set up a backup process. Relying solely on AOF is not a backup (it can get corrupted by a disk issue). Periodic RDB snapshots shipped to external storage are safer.
- **Ignoring the distinction between cache and DB:** Using the same Redis deployment for caching and as a primary data store without different configurations. For example, you might have maxmemory with an eviction policy turned on for the cache use-case - but if that instance also holds data that shouldn't be evicted, you're in trouble. One or the other will suffer. It's an anti-pattern to mix these roles; use separate Redis instances or at least separate databases (and be very careful with eviction settings).
- **Restarting or upgrading Redis without planning:** Unlike stateless services, Redis nodes have state. An anti-pattern is treating them with the same upgrade strategy as a stateless app (e.g., just killing all at once). Always do rolling restarts (one node at a time) to avoid full outages. In cluster or sentinel setups, failover nodes as needed during maintenance.
- **Over-reliance on Redis persistence without understanding it:** For instance, using RDB snapshotting infrequently (say every 30 minutes) and thinking that's enough. If a crash happens, you lose up to 30 minutes of data. Or using AOF every second and not understanding that a sudden power loss could still drop a second of writes. If your use-case cannot tolerate that, the anti-pattern is not using appendfsync=always or acknowledgments to replicas (WAIT). Always align Redis persistence configuration with your data loss tolerance.
- **Split-brain scenarios not considered:** In sentinel setups, if the network partitions, you might get two masters (split-brain). The anti-pattern is not running an odd number of Sentinels or not having a quorum. Ensure at least 3 Sentinel instances for a robust setup. Similarly for cluster, if you deploy an even number of master shards without replicas, a partition can freeze the cluster (lack of majority).
- **Lack of monitoring on failovers:** You should get alerted when a failover happens (it means something went wrong with the primary). An anti-pattern is failing to investigate such events - they could indicate underlying issues (e.g., out-of-memory, network flakiness) that need fixing. If left unchecked, you might have multiple failovers and potential data divergence or performance hits.

## Observability and Monitoring

### PostgreSQL (with Prisma)

- **Best Practices:**
- **Monitor database metrics continuously:** Key metrics include CPU utilization, memory usage (especially cache hit ratio of Postgres - you want a high cache hit %, indicating most reads come from memory), disk IOPS (to spot if queries cause a lot of reads/writes), replication lag (if using replicas), number of connections, and locks or wait events. In AWS RDS, CloudWatch provides many of these (CPU, FreeableMemory, ReadIOPS/WriteIOPS, ReplicaLag, etc.) which you should alarm on if they go beyond thresholds.
- **Use query monitoring and logging:** Enable slow query logging in Postgres. For example, set log_min_duration_statement to a threshold (e.g. 200ms) to log queries slower than that. This helps catch inefficiencies. Tools like pgBadger or AWS Performance Insights can aggregate this. Prisma can also log queries (by enabling logging in Prisma Client) - this is useful in dev to see what queries are generated and in prod for slow query tracing (though be careful to not log an overwhelming amount).
- **pg_stat_statements and performance analysis:** Install the pg_stat_statements extension on Postgres (RDS enables it by default when Performance Insights is on). This view shows the frequency and average time of each query signature. It helps identify the most expensive queries overall, not just individually slow ones. Regularly review this to see if new code introduced an inefficient query pattern.
- **Application-level monitoring of DB calls:** Instrument your microservices to collect metrics on database calls: e.g., how many queries per request, latency of each DB query, error rates, etc. APM tools (like New Relic, Datadog, AppSignal) often have integrations for ORMs or for tracking SQL queries. This can tie a slow user request to a specific slow database query, bridging the gap between service and DB monitoring.
- **Tracing and correlation:** In distributed tracing (with tools like OpenTelemetry, Jaeger, Zipkin), include database spans. Prisma doesn't natively integrate with tracing yet (as of mid-2020s) but you can manually instrument around your DB calls. This helps when a single request goes through multiple services and DBs - you can pinpoint that e.g. Service A spent 50ms in its DB, Service B spent 200ms in its DB, etc.
- **Error logging and alerts:** Log database errors with enough context (but not sensitive data). For example, if a query fails due to a constraint violation or deadlock, log the query (or at least the involved table/operation) and the error code. Set up alerts for frequent errors - e.g., if you start seeing a spike in deadlock errors or connection errors, that should alert the on-call, as it might indicate a saturation or misbehavior in the system.
- **Monitor Prisma Client itself:** Prisma can throw specific error codes (like P2002 for unique constraint violation, P2024 for timeouts, etc.). Track these occurrences. Also, Prisma's internal connection pool metrics (number of in-use vs idle connections) are not exposed by default, but if you notice high latency, investigate if it's waiting for free connections (pool exhaustion).
- **Resource and OS monitoring:** If you manage Postgres on VMs or containers, also monitor OS metrics (CPU steal, swap usage, open file descriptors) and Postgres process metrics (like context switches, which can indicate too many connections). Ensure the system is not swapping (that kills performance). In Kubernetes, use liveness/readiness probes for your Postgres container and have proper resource requests/limits to avoid eviction or throttling.
- **Auditing access:** For security and also observability of unusual activity, monitor logins and permission changes. AWS CloudTrail can alert on RDS configuration changes or login attempts. If a service normally uses 10 connections and suddenly uses 100, that might indicate a bug (connection leak or unexpected load).
- **Antipatterns:**
- **Lack of monitoring/dashboarding:** Flying blind is dangerous. An anti-pattern is not having a dashboard for your database performance or waiting until an incident to try to figure out metrics. If you don't have basic graphs (CPU, connections, latency) and alerts, you're in reactive mode.
- **Logging too much or too little:** Two extremes: not logging any queries (so you can't diagnose issues) - or logging every single query in detail in production, which can overwhelm IO and storage, and possibly expose sensitive data. For example, setting log_statement = 'all' in production is overkill (and can even log passwords in some statements if not careful). A balanced approach (like slow query logging or sampling) is needed.
- **Ignoring alert signals:** If your monitoring is in place but the team ignores alerts (or silences them without action), that's an anti-pattern. E.g., consistently high CPU on the DB or frequent cache misses should prompt investigation before it becomes a fire.
- **No connection pool monitoring:** If using RDS Proxy or PgBouncer, not monitoring their stats (like how many connections they are proxying, how often connections are being re-used) can hide issues. For instance, if your app is frequently opening new connections because it doesn't reuse the ORM instance properly, you might see spikes that aren't obvious without looking at the pool.
- **Siloed monitoring - not correlating with app metrics:** Only monitoring the database in isolation. This can lead to finger-pointing ("DB is fine, must be the app" or vice versa). It's a bad practice to not correlate the two. For example, see if a spike in HTTP 500 errors correlates with a spike in DB errors, or if increased latency in one service correlates with high CPU on its DB - that correlation is key to quick diagnosis.
- **Manual log analysis:** Relying on manually grepping DB logs when something goes wrong is slow. Not using any log aggregation for Postgres (like sending logs to CloudWatch, Splunk, or ELK) is an antipattern - in an incident you might not even have easy access to the DB server logs.
- **Not monitoring read replicas:** If you have read replicas, ignoring their performance is a mistake. They can fall behind or have replay lag. If a read replica gets far behind, your read queries may return stale data or, in worst case, the replica might be useless in failover. Always monitor replica lag.
- **No monitoring of Prisma migration or dev actions on prod:** If someone accidentally runs a heavy migration or a debug query in production, will you know? It's good to have role-based query logging - e.g., log if an admin or an unusual role runs a query. Not doing so is an antipattern as it might hide human errors or malicious actions.
- **Overlooking capacity trends:** Monitoring isn't just for real-time. An anti-pattern is not analyzing trends - e.g., disk space gradually filling with WAL files or data, connections slowly increasing as traffic grows, etc. By observing trends, you can plan scaling or cleanup before it becomes a problem.

### Redis

- **Best Practices:**
- **Key Redis metrics to monitor:** Track memory usage (used_memory from INFO), memory fragmentation ratio, CPU usage, network throughput, and eviction counts. A growing eviction count means your cache is thrashing or memory is insufficient. Monitor the cache hit rate (Redis INFO provides keyspace_hits and keyspace_misses) - this tells you how effective your cache is. If misses are high, maybe the dataset doesn't fit or TTLs are too short.
- **Latency and command monitoring:** Redis is typically sub-millisecond, so if you observe higher latency, investigate. Use the Redis Slow Log (slowlog-get) to capture commands that exceed a threshold (default 100ms). If you see slow log entries, identify those commands and why they're slow (maybe large data returns or blocking operations). You can also use tools like Redis Monitoring (redis-cli MONITOR in dev, or Redis Insight tool) to see live commands (not in prod, too heavy).
- **Monitor number of connections and client status:** Use CLIENT LIST or metrics to see how many connections are open. If the count is near the max clients or if many clients are in blocked state (for BLPOP, etc.), that's useful info. In AWS, CurrConnections is available. A sudden drop could mean a network partition; a sudden spike could mean a connection storm.
- **Track replication and persistence health:** If using replication, monitor replication lag (replication offset or slave delay). Redis INFO replication section shows if a replica is catching up. Alert if replication link is down or lagging significantly. If using AOF, monitor the AOF write time and size - a rapidly growing AOF might need a BGREWRITEAOF soon. If a BGREWRITEAOF or BGSAVE fails (Redis logs this), that should alert you since durability is compromised.
- **Integrate with centralized logging/metrics:** Send Redis logs to a central system. Redis logs events like persistence errors, failovers, or maxmemory eviction events. For metrics, use a Prometheus Redis Exporter or similar to scrape all INFO stats and custom metrics. There are also cloud-specific monitors (e.g., AWS CloudWatch has an ElastiCache metric for CPU, memory, swap, eviction, etc.). Set thresholds: e.g., alert if memory usage hits 90% of max, or if evictions > 0 for a primary data store.
- **Dashboards for Redis usage per microservice:** If multiple services use one Redis, try to segment metrics by command (some exporters break down usage by command types). For example, if one service uses mainly GET/SET and another uses mainly list operations, seeing ops/sec per command type can identify which service's usage is dominating.
- **Use Redis-specific monitoring tools if needed:** For deep analysis, Redis Insight (by Redis) or other third-party tools can provide visualization of key distribution, memory per key pattern, etc. This can help in optimizing data models (e.g., if one key is huge or one pattern dominates memory).
- **Simulate cache churn and measure:** In staging, you can simulate a cache flush or large churn and see how the system behaves (cache miss storm). Monitor how quickly the cache recovers (fill rate) and if the database can handle the miss traffic. Observing these scenarios can be part of resilience testing but also gives observability into whether your monitoring would catch it (e.g., a sudden drop in hit rate).
- **Set up alerts for abnormal events:** For example, if you suddenly see **connected clients** drop to zero, that could mean a Redis outage (all clients disconnected) - alert immediately. If **cache miss rate** triples suddenly, maybe a deploy caused bad cache usage - worth investigating. If a **replica** falls behind or goes offline - alert, since you're temporarily without HA. Essentially, define what "normal" is and alert on deviations.
- **Antipatterns:**
- **No visibility into caching effectiveness:** Not measuring cache hits/misses is an anti-pattern. You might be running Redis but not actually getting much benefit if the miss rate is high. Or you might inadvertently be caching useless data. Without metrics, you wouldn't know if you need to adjust TTLs or memory.
- **Ignoring memory warnings:** Redis will log if it's close to maxmemory or if it starts evicting keys. An anti-pattern is to ignore these warnings until keys start disappearing (or OOM errors occur). By then, users might be affected (e.g., session data evicted). Always react to memory pressure signals - either by scaling out, increasing memory, or tuning usage.
- **Monitoring only at host level, not Redis level:** Relying solely on system metrics (CPU, RAM usage of the OS) without looking at Redis internals. For example, OS could show plenty of free RAM, but Redis might be at its maxmemory and evicting. Or CPU might be low, but you could have high lock contention inside Redis due to some heavy command. Without Redis-specific metrics, these scenarios are missed.
- **Not tracking changes after deployments:** If a code deployment dramatically increases Redis traffic (e.g., a new feature that does more caching or more queue ops), an anti-pattern is failing to notice and adjust. This could prematurely exhaust resources. Always compare before/after deploy metrics.
- **No alert on replication status:** If a replica is months behind or disconnected, you effectively have no HA. Not alerting on that is risky. Some teams have failed over to a replica only to find it far out of date - a situation that monitoring should prevent.
- **Treating Redis as a black box:** Not using the wealth of info from INFO command. That command gives insight into everything from memory fragmentation (if fragmentation is very high, you might consider a restart to defragment memory or check for memory leaks in Redis allocator) to key count per DB. Ignoring this info is an opportunity lost.
- **Over-alerting (crying wolf):** Conversely, an anti-pattern is alerting on every minor Redis fluctuation (like a single slow command) and causing alert fatigue. Tune your alert thresholds to be meaningful (e.g., X slow commands in Y time, or consistently high latency over a period, etc.). If your team gets paged too often for non-issues, they might start ignoring alerts.
- **Not logging consumer lag in streams or queues:** If you use Redis Streams or lists for background jobs, not monitoring the length of pending messages or list length is bad. If consumers fall behind, the list length will grow - if you don't watch it, you might run out of memory or violate SLAs (jobs taking too long). Similarly, for Streams, each consumer group has a pending entries list (PEL); if that grows or if messages stay pending too long, it indicates a stuck or slow consumer.
- **No monitoring of key churn or usage patterns:** If a bug causes a microservice to unintentionally flood Redis with keys (e.g., not using a proper key prefix and creating millions of entries), you might not notice until memory is exhausted, if you aren't monitoring the number of keys or memory usage by keys. It's an anti-pattern to not have any insight into how many keys are being created or expired over time.

## Error Prevention and Safe Integration Patterns

### PostgreSQL (with Prisma)

- **Best Practices:**
- **Use database transactions for atomic operations:** When a business operation involves multiple SQL statements (insert/update several tables), always wrap them in a transaction (prisma.\$transaction(\[...\])). This ensures that either all changes succeed or none do, preventing partial updates that could leave data inconsistent. For example, if a payment service debits one account and credits another, both should happen in one transaction or not at all.
- **Enforce data integrity at the database level:** Define constraints such as foreign keys, unique constraints, and check constraints in PostgreSQL to catch errors early. Even if each microservice owns its data (no cross-service FK), within a service's schema these constraints prevent logical errors. Prisma will throw exceptions if a DB constraint is violated (e.g., a duplicate key on create), which you can catch and handle (e.g., return a 409 Conflict if trying to create a record that already exists).
- **Implement safe migration practices:** In microservices, you deploy updates regularly. Always design schema migrations to be backward compatible during deployment rollouts. That means: do not drop a column or make a breaking change while old code might still be running. Instead, add new columns or tables, update the code to use them (while possibly writing to both new and old schema if needed), then in a later deployment remove or drop obsolete parts once no code uses them. This **expand-and-contract** pattern avoids downtime or runtime errors. Use Prisma Migrate to generate and apply migrations in a controlled way.
- **Automate schema management:** Have migrations run as part of your CI/CD pipeline or on service startup in a controlled manner (with appropriate locking to avoid races). The best practice is one source of truth for schema (the Prisma schema). If two services share a database (not ideal, but if so), coordinate changes - e.g., a shared migrations repository or one service designated to apply migrations for shared schemas.
- **Idempotent operations and retries:** Design microservice interactions such that if an operation is performed twice, the end state is the same as if it were done once. This way, if a network call fails and is retried, or if a transaction is retried after a deadlock, you don't end up with duplicate or contradictory data. For instance, use **UPSERT** (on conflict) for certain create-or-update scenarios, or check if an operation has already been applied (via a unique request ID) to avoid double-processing.
- **Outbox pattern for cross-service consistency:** When one service needs to trigger an action in another as part of a transaction (e.g., order service creates an order and needs to notify inventory service), use an outbox table. The order service, in the same transaction as inserting the order, inserts an "order_created" event in an outbox table. A separate process or thread reads from this table and publishes to a message broker. This ensures the event is only published if the transaction commits. This pattern prevents missing or duplicate events in case of failures and keeps cross-service communication reliable.
- **Saga pattern for distributed transactions:** For more complex multi-step processes (like an order that involves inventory, payment, shipping services), use a saga - essentially a sequence of local transactions with compensating actions on failure. Each service performs its step and if one fails, previously completed steps are undone via compensations. This prevents permanent partial completion when no single DB transaction can encompass all steps.
- **Input validation and ORM-level guards:** Validate data at multiple layers - UI, service, and database. For example, ensure an email field is a valid email format in the service before it even goes to the DB (although the DB might also have a constraint or length limit). Use Prisma's schema to enforce data types and perhaps use database-side CHECK constraints for critical invariants (like value ranges).
- **Defensive coding around DB results:** When querying, assume the data might not be present or might violate assumptions (especially if the data could be mutated by other processes). For example, if you SELECT FOR UPDATE a row to increment a value, handle the case it doesn't exist (maybe someone deleted it) by deciding to insert or error out. Essentially, always check the results of queries and handle 0 rows affected cases.
- **Graceful degradation on partial failures:** If your microservice can still function somewhat without full DB access, handle that. For instance, if a read query fails, you might return cached data or an error specific to that component rather than crashing the whole service. In UI terms, show partial data with a warning. At the service level, maybe serve a stale response from cache if DB is down (read-through cache scenario). Not always possible, but consider if there are safe fallbacks.
- **Antipatterns:**
- **Sharing databases between services for integration:** E.g., Service A directly reads/modifies Service B's tables to avoid an API call. This is a big anti-pattern in microservices - it creates tight coupling, bypasses validation/business logic of the owning service, and makes it extremely hard to change schemas safely (since another service might break). Instead, services should communicate via APIs or messaging, not via each other's databases.
- **Lack of error handling around DB operations:** Assuming every DB call will succeed and not catching exceptions. This leads to crashes or inconsistent state when something does go wrong (and in distributed systems, it eventually will). For instance, not handling a unique constraint violation (P2002 in Prisma) could result in a 500 error to client instead of a meaningful message. Or not handling a transaction deadlock could mean a crucial operation just fails silently. Always catch and handle expected errors (with retries or compensations) and log unexpected ones with context.
- **Long-running business logic within transactions:** Doing significant computation or waiting on external calls (like an HTTP request to another service) while a DB transaction is open. This is an anti-pattern because it holds locks and could block other operations, increasing chances of deadlocks and timeouts. Keep transactions short; do external calls either before or after the transaction, not during.
- **Overloading the database with multi-service concerns:** Using database triggers or procedures to implement cross-cutting logic that spans services. For example, a trigger on Table X tries to call a REST API of another service or write to another service's table. This is mixing boundaries and is hard to manage (and often impossible if the other DB is separate). Keep such logic in the service layer; the database should enforce integrity for its own service's data only.
- **No standard for database errors:** If each developer handles errors differently (or not at all), it results in unpredictable behavior. For example, failing to standardize transaction retry logic - some parts of the code may retry on deadlock, others might not, leading to inconsistent reliability. Not implementing a few global error-handling strategies is an anti-pattern. Instead, have guidelines: e.g., "on serialization failure, retry up to 3 times", "handle unique constraint by returning a specific error code to the client", etc.
- **Simultaneous schema changes and code changes with no coordination:** Deploying a new service version that expects a DB change _before_ running the migration, or vice versa (running a destructive migration before all services are updated). This can cause immediate failures. e.g., removing a column that old code still uses = runtime errors; deploying code that expects a new column that isn't there = errors. Always coordinate via backward compatibility as mentioned. Not doing so is a common anti-pattern leading to production incidents.
- **Not using constraints/ relying only on app logic:** For example, not defining a UNIQUE constraint because "the app ensures no duplicates". This is risky, especially in distributed systems or when multiple app instances run. A bug or race condition can slip through duplicates. Relying solely on app-level checks is an anti-pattern; the database should be a second line of defense.
- **Ignoring isolation anomalies:** Running everything in the default isolation (Read Committed) is fine for most, but if you have a case where phantom reads or write skew could occur and you don't consider it, you might get incorrect results under contention. One anti-pattern is to not analyze if your transactions need a higher isolation or explicit locking. This can lead to subtle bugs (e.g., double-spending type issues where two transactions see stale data and both perform an action). Always consider if you need SERIALIZABLE or at least REPEATABLE READ for certain critical sections - and use it if so, or use explicit locks (SELECT  FOR UPDATE).
- **Database as the messaging system:** Using DB tables as a makeshift queue between services (like polling a table for new entries) instead of a proper message broker or event bus. While it can work for small scale, it couples the services via the database and adds load to the DB. If done, it should be very limited and short-term. Otherwise, it's better to use something like RabbitMQ, Kafka, or even Redis for cross-service messages.

### Redis

- **Best Practices:**
- **Cache-Aside Pattern (Lazy Loading):** When using Redis as a cache, follow the cache-aside approach: on a cache miss, load from the primary data source (e.g., Postgres), then populate the cache. This ensures the cache only contains data that has been requested, and it keeps the cache loosely coupled. Write-through caching (updating cache on DB write) can also be used, but cache-aside is simpler and avoids caching data that might never be read.
- **Proper cache invalidation:** Invalidate or update cache entries when the underlying data changes. If your microservice updates the database, it should also delete or update the relevant Redis keys (unless using short TTLs where you accept eventual consistency). Use consistent key naming so that keys can be identified and invalidated (e.g., all cache keys for user 123 start with user:123:\*). Invalidation can also be done via Pub/Sub notifications to other services that use the cache (e.g., an "entity updated" message causes subscribers to evict that cache entry).
- **Preventing cache stampedes:** Use techniques to avoid many requests thundering to the DB on cache expiry. For example, **lock** or **semaphore**: when a cache miss occurs for a popular key, have one request populate it while others wait. This can be done by using a Redis lock (e.g., SETNX a lock key) or using the "double-check locking" pattern. Another technique is **external recompute jitter**: randomly stagger cache expirations (or use different TTL for similar keys) so they don't all expire at once. Some libraries implement cache stampede protection by temporarily marking a just-expired key as "in progress" and serving stale data to others until the new value is computed.
- **Distributed locks for critical sections:** If multiple instances of a microservice must not do something concurrently (e.g., process the same user twice), Redis can be used for a distributed lock. Use a robust algorithm like Redlock (which involves setting a key with a short expiry and requiring a majority of instances in a Redis cluster to grant the lock). Always set an expiry on locks (to auto-release if the holder dies) and check that you still hold the lock when releasing (to avoid releasing someone else's lock). This prevents concurrency issues across nodes.
- **Reliable queue processing:** If using Redis lists as a queue, use the **BRPOPLPUSH** pattern (or the newer variant RPOPLPUSH + BRPOP or just XREADGROUP for Streams). This pattern: atomically move an item from a main queue to a processing queue. The worker then processes the item, and upon success, removes it from the processing queue. If the worker dies, the item remains in the processing list and can be re-claimed later (by a recovery process that checks the processing queue for stalled items). This prevents losing the item if a consumer dies after removal but before processing. Alternatively, use Redis Streams which have built-in acknowledgment (and pending entry list) for reliable delivery.
- **Use blocking operations for waiting:** For example, consumers should use BLPOP/BRPOP on lists (or XREADGROUP BLOCK on streams) instead of polling in a tight loop. This reduces CPU usage and latency. It's straightforward: a worker can block on a list with a timeout. Polling (doing GET in a loop) is an anti-pattern that wastes resources.
- **Idempotency and de-duplication:** In message processing (queues, streams, or pub/sub events), design consumers to be idempotent whenever possible. That way, if a message is delivered twice (which can happen in certain failure cases or with at-least-once delivery semantics), it won't cause double effects. For example, include a unique ID with events or jobs and have the consumer check if it's seen that ID before (store processed IDs for a short time, or use a dedup mechanism in streams with the last_id).
- **Use Consumer Groups for multi-consumer scenarios:** With Redis Streams, leverage consumer groups to have multiple consumers share the workload of a stream (similar to a Kafka consumer group). This also allows acking of messages and tracking pending ones. For simple fan-out (all consumers get all messages), pub/sub is fine, but for work queue sharing, consumer groups are the way to go.
- **Small messages and data references:** As noted, keep messages small in pub/sub or queues. If you need to convey a lot of data, consider storing it (e.g., in a DB or object storage) and sending a reference (ID or URL). This keeps Redis lean and fast. Similarly, for cache, don't store gigantic blobs if you can avoid it (store a reference if the blob can be fetched elsewhere).
- **Separate concerns with multiple Redis deployments or databases:** Use different Redis instances (or at least logical DBs) for different purposes to avoid interference. For example, session caching vs job queues have different access patterns and durability requirements. By separating them, you can tune each (and also not worry that a slow consumer on the queue will affect cache response time). If using the same instance, at least use different DB indexes (though note: Redis logical DBs share CPU and memory, just separate key spaces).
- **Test integration in staging:** Reproduce failure modes like lost lock (simulate a crash with a lock held), or a job taking too long and appearing twice, or heavy load causing cache misses - and verify your patterns hold up (e.g., no double processing, performance degrades gracefully). This will validate your safe integration patterns in a controlled way.
- **Antipatterns:**
- **Using Pub/Sub when a queue or stream is needed:** Pub/Sub drops messages if subscribers aren't listening. Using it for tasks that must be completed is an anti-pattern. For instance, sending an order confirmation via pub/sub - if the email service is down, that message is lost. The better approach is a queue (or Redis Stream) where the message stays until processed. Pub/Sub is for ephemeral, real-time notifications where you don't care about persistence.
- **Ignoring concurrency in consumers:** Assuming that because you have a single-threaded Redis, your consumer logic doesn't need to worry about race conditions. If you have multiple consumers on different services (or even threads), you can still process the same item twice if you don't guard properly. Not using the reliable queue pattern (i.e., just doing BLPOP and processing) can lead to lost messages if a consumer dies at the wrong time. Similarly, not handling that two different services might try to create the same cache entry simultaneously can cause duplicate heavy computations. Always consider adding locks or atomic operations where needed.
- **Infinite TTL or no eviction on caches that mutate:** If you cache something that can change, and you never expire or update it, you'll serve stale data indefinitely - a glaring anti-pattern. Developers sometimes cache aggressively and forget invalidation, leading to bugs where the UI shows outdated info. Every cache entry should have an invalidation strategy (explicit purge on update or a reasonable TTL that bounds staleness).
- **Overusing Redis as a datastore beyond its design:** Using Redis for transactional data that requires complex relational consistency (instead of a proper DB) can get you in trouble. For example, implementing a financial ledger in Redis might be an anti-pattern unless carefully designed, because you lack multi-key transaction safety and durability out-of-the-box. If you start implementing elaborate multi-step pipelines with Lua to mimic SQL transactions, reconsider if Redis is the right tool or if you should use a database for that part.
- **No fallback for cache aside:** If your code expects data in cache and doesn't find it but also doesn't fetch from DB (e.g., returns null or error immediately), that's an anti-pattern. Cache misses are normal; always handle them by fetching from source. Similarly, if Redis is down, the service should have a mode (even if slower) to get data from the DB directly. Crashing or returning errors just because the cache isn't available defeats the purpose of resilience.
- **Directly exposing Redis to untrusted clients:** Some people attempt to use Redis as a primary database accessible from front-end or third-party systems (because it's fast). Without an application layer, this is dangerous (security, data validation). Also, exposing complex data structures directly can tie your clients to your internal implementation (e.g., they directly read a Redis hash). It's an anti-pattern to bypass the service and let outside access to Redis in a microservice architecture.
- **Not cleaning up ephemeral data or keys:** If your service writes temporary keys (locks, intermediate results, etc.) and doesn't remove them or expire them, Redis can get cluttered or filled. For example, forgetting to release a lock (no expiry) can deadlock processes; forgetting to delete a "processing" key if a job fails means it never gets retried. Always ensure ephemeral keys have a TTL or are cleaned up in a finally block of your code. Not doing so is an anti-pattern leading to resource leaks.
- **Mismanagement of data consistency between Redis and DB:** For instance, updating the database but not the cache (cache becomes stale), or updating cache but not properly persisting to DB (cache holds new data that DB doesn't have, which might be lost on restart). A classic anti-pattern is "write-behind" caching where you put data in cache assuming it will eventually go to DB - if not carefully done, a crash can lose data. Prefer write-through (update DB then cache, or use a library that writes to both). If using write-behind (async DB update), make sure to queue the writes and handle crashes by replaying the queue.
- **Trusting data in Redis blindly:** If using Redis as a shared cache or bus between services, do not assume the data format is always as you expect - version mismatches could mean one service writes a slightly different structure. It's good to include version info in keys or hash fields. An anti-pattern is to have no contract, leading to one service breaking another via Redis. Always treat cached data as potentially stale or in need of validation (e.g., include a simple checksum or version number).

## Load Balancing and High Availability

### PostgreSQL (with Prisma)

- **Best Practices:**
- **One database per microservice principle:** Each microservice should ideally have its own dedicated database (or logical schema). This not only decouples them (as mentioned) but allows you to scale and tune databases independently. For example, a read-heavy service can have more replicas; a write-heavy service can be on an optimized instance. It also confines failures - if one database goes down, others are unaffected.
- **Read-Write splitting for scaling:** If your microservice has significantly more read traffic than writes, use a primary-replica setup and direct reads to replicas. While Prisma doesn't natively route queries to replicas, you can instantiate a second Prisma Client pointing at the replica endpoint for read queries. Some teams use this pattern: Prisma (or another lightweight client) for reads from a readonly replica and Prisma for all writes to primary. Ensure the application logic is aware of eventual consistency (reads might be slightly behind writes).
- **Connection pool management and load balancing:** In Kubernetes or a dynamic environment, the number of app instances can change. Rather than each maintaining a full pool to the DB (which could overwhelm it), consider a connection pooler like PgBouncer. PgBouncer can sit between your apps and Postgres to reuse connections efficiently. AWS's RDS Proxy is a managed solution that does similar pooling and can also help with failovers (keeping connections alive). Load balancing in Postgres typically refers to balancing read traffic across replicas - you might use a DNS that rotates among replicas for read-only connections, or better, have the app handle multiple read targets. For write traffic, it all goes to the primary - ensure only one primary at a time (except in multi-master systems like some Aurora modes or CockroachDB which are different databases entirely).
- **High Availability setups:** Use mechanisms appropriate to your deployment:
  - **Cloud (AWS/Azure/GCP):** Use their managed HA (Multi-AZ on AWS RDS gives automatic failover within ~30-60s typically). Aurora offers faster failover (often <30s) and read scaling out-of-the-box.
  - **On-prem/DIY:** Use a cluster manager like Patroni, which uses etcd or ZooKeeper to coordinate Postgres nodes and handle automatic failover. Ensure you have fencing so that the old primary doesn't accidentally accept writes after a failover (to avoid split brain).
  - **Connection string management:** Use a virtual IP or DNS that points to the current primary. For example, AWS provides an endpoint that always points to the writer in Aurora. That way, clients always connect to the endpoint and don't need to know which node is primary.
- **Exponential backoff on reconnects:** When a database goes down or during failover, dozens of microservices will start retrying. If they all retry in a tight loop, the new primary (once up) gets bombarded, possibly knocking it over again. Implement exponential backoff with jitter in your retry strategy for database connections. This is often handled by the driver/ORM: e.g., have a base retry wait (100ms) that doubles each time up to some max, with some random factor. This staggers reconnections and gives the DB time to recover.
- **Capacity planning for peak load:** Ensure the database can handle the highest expected load, plus some headroom. Load balancing microservices without considering DB capacity is dangerous (you might scale app instances to handle traffic, but then all hit the same DB). Use autoscaling or scale-out for the DB tier too if possible (Aurora Serverless v2 can autoscale in some range). If not, at least know the limits and don't scale app beyond what DB can handle - or you'll need to implement backpressure in the app.
- **Sharding for very high scale:** If one service's data becomes so large or high-throughput that one DB node is insufficient, consider sharding that service's database by some key (e.g., by customer region, by tenant ID, etc.). Each shard is a full Postgres, and the service logic routes queries accordingly. This is complex, so do it only as needed. Alternatively, consider if a different data store fits (NoSQL or NewSQL) if scaling Postgres is too hard for that use-case.
- **Global deployments:** If your microservices are deployed in multiple regions, decide how to handle data locality. A common approach: keep writes local to one region (one primary) and have cross-region read replicas for local reads. This introduces consistency delays cross-region. If multi-region active writing is needed, consider a database that supports multi-master or use an application layer conflict resolution. With Postgres, you might use tools like Bucardo or custom conflict handlers, but these get complicated. Often, a simpler approach is to partition users by region (so each region has its own primary for its set of data).
- **Automation and DevOps for HA:** Use infrastructure as code (Terraform, etc.) to configure read replicas, failover priorities, etc. Have scripts or Lambdas for failover if not using managed solutions. Regularly update your connection strings or driver settings to remove retired replicas, etc. Ensure your team knows how to failover manually too, in case automation fails.
- **Multi-tenancy within a service DB:** If a microservice is multi-tenant (serves many clients), consider strategies to balance load: e.g., if one tenant is extremely large, you might isolate them in their own DB or schema (a form of sharding). Load balancing then occurs by segregating heavy tenants. The microservice can be aware of this and connect to the appropriate DB instance per tenant. This prevents one tenant from overloading the primary DB for all.
- **Antipatterns:**
- **Scaling microservices without scaling the database:** It's an anti-pattern to think "we can handle more traffic because we added more app servers" while the database remains a singleton bottleneck. This often leads to high DB CPU or locks and then the whole system slows down. Always include the database in your scaling strategy.
- **No read replicas while reads are bottlenecking:** If your primary is overloaded with read queries and you have nearly zero replica usage, that's a missed opportunity. It's an anti-pattern to not offload reads that can tolerate slight staleness. On the flip side, blindly using replicas without checking lag can return outdated data; know your data consistency requirements.
- **Using round-robin DNS for writes:** Some might be tempted to set up multiple writable DB nodes and distribute writes via DNS or a load balancer. This doesn't work for Postgres (unless using something like logical replication with conflict resolution, which is advanced). Writes must go to one primary to maintain consistency. An anti-pattern is trying to "load balance" writes between two independent DBs - you'll end up with divergent data.
- **Ignoring the connection limit and saturation:** Each Postgres has a max connections setting (often 100 by default, can be increased but each connection eats memory and context). If you deploy 50 microservice instances each with pool of 10, that's 500 connections - possibly too many. Running at or beyond the connection limit causes queueing and failures. Anti-pattern: not tuning pool sizes or using a proxy when needed, and hitting the connection limit regularly.
- **Not leveraging connection pooling in serverless:** If using AWS Lambda or other serverless with Postgres, each cold start can open new connections. This is an anti-pattern if you expect high concurrency - you can exhaust the DB. Solutions include using RDS Proxy or limiting concurrency. Not doing so has caused many real incidents where databases were overwhelmed by lambda bursts.
- **Monolithic database for all services (again):** It also appears here because of HA - one big database for all microservices is harder to make highly available (it's a giant single point of failure). If it goes down, many services are down. Even if using HA techniques, recovery of a huge monolithic DB might be slower than many smaller ones. So this anti-pattern affects both performance and HA.
- **Having stateful load balancing without sticky sessions when needed:** If your app uses something like session tables or advisory locks in the DB, it might matter which app instance handles which user. Typically we avoid that by keeping state in the DB or external store (so any instance can serve any request). But if not, and you do need affinity, not configuring that at the load balancer is an issue. Fortunately, for microservice-to-DB this usually isn't an issue because the DB isn't behind a load balancer in the same sense; but in general, ensure that state expectations between app and DB are met. (E.g., if you use pgBouncer in transaction pooling mode, don't use session-based features like temp tables because they might go to a different connection).
- **Failing to prepare for failover lag or read-after-write issues:** After a failover, a newly promoted replica might be a few seconds behind the old primary at the moment of promotion. Most systems accept this minor data loss or delay, but it should be acknowledged. If you absolutely cannot lose a transaction, you'd need synchronous replication (where a transaction commits only when replica has it), which impacts performance. Not understanding this trade-off is an anti-pattern - either you might lose some data unexpectedly, or you enable sync mode and then get hit with slowdowns. Make an intentional choice and document it.
- **Manual, untested failover processes:** Relying on someone to manually change connection strings or promote a replica under duress is risky. If you haven't automated or at least practiced it, the chance of error is high. That's an anti-pattern in HA - always automate where you can, and practice drills for the manual steps.
- **Not utilizing multi-AZ (cloud) or multi-rack (on-prem):** If your primary and replica are in the same rack or AZ, a single outage (power, network partition) takes both out. An anti-pattern is to configure HA components too close together due to convenience or cost. It defeats the purpose of HA. Always spread out HA components to reduce correlated failure.

### Redis

- **Best Practices:**
- **Redis Cluster for scaling and fault isolation:** Use Redis Cluster mode to partition data across multiple nodes (shards). This not only scales throughput by parallelizing across shards but also limits the impact of a single node failure (only a portion of keys are affected, and each shard has replicas for failover). Cluster mode is recommended for datasets that can't fit on one node or when ops/sec needs exceed a single node's capacity. The cluster will also handle moving slots around during scale-out (though during resharding there's some overhead).
- **Sentinel for primary/replica failover (if not clustering):** If you use a single Redis (or a few independent Redis instances), use Redis Sentinel to monitor and automatically failover a replica to primary if the primary goes down. Ensure an odd number of Sentinels (minimum 3) so they can form a quorum. Also configure clients with the list of Sentinel addresses - they will query Sentinel to get the current primary's address and switch to it on failover. This provides HA without manual intervention.
- **Scale writes through sharding:** Since a single Redis is single-writer (one thread), if you need more write throughput, you must shard. Either use Redis Cluster or do client-side sharding (e.g., consistent hashing on key). The latter can be done with tools like Twemproxy. The idea is to distribute keys so that no one node is overwhelmed. For example, if one node reaches 80% CPU and others are low, you might need to redistribute some keys or add shards.
- **Geographical distribution if needed:** If your microservices are across regions and need a cache in each, it might be better to have separate Redis instances per region (and possibly a way to sync critical data between them asynchronously). Redis does not do multi-master across regions. You can replicate one way (primary in one region, replica in another), but high latency for writes. Evaluate if cross-region caching is needed or if each region can build/cache its own data. Avoid doing cross-region calls to a single Redis - that adds latency and complexity; instead deploy one per region if possible.
- **Use DNS or discovery for Redis endpoints:** For HA setups, use a stable endpoint that can redirect to current primary. AWS Elasticache provides a Configuration Endpoint for cluster mode and a Primary Endpoint for single primary setups, which will point to the right node even after failover. If self-managed, consider using a virtual IP or a service registry that clients query. This way, clients don't need to manually update when a failover happens.
- **Client-side load balancing for cluster:** In Redis Cluster, the client will get a map of slots to nodes and route requests accordingly (no need for an external load balancer). Just ensure you initialize the cluster-aware clients with multiple seed nodes (so that if one is down, it can still discover the cluster via another). Many modern clients do this automatically.
- **Limit per-shard load by key design:** If a particular pattern might overload one shard (e.g., keys are all prefixed with "hot:" and those all hash to the same shard), consider adjusting the key hashing by including a shard key or hashtag in Redis terms. Redis Cluster uses part of the key in {...} as the hash input. By choosing that intelligently, you can ensure a better distribution. For example, user:{userid}:sessions ensures that all keys for one user go to the same shard (which can be good for locality) but if one user is extremely active, that one shard gets hit - maybe you'd rather distribute by something else. Balance the design to avoid single-shard hotspots.
- **High availability in Kubernetes:** If running Redis in K8s, use StatefulSets for stable network IDs, and possibly an operator (like Redis Operator or Helm chart with sentinel) to manage failovers. Ensure that if a pod dies, another can come up with the same data (using persistent volumes). If using cluster, have anti-affinity rules so that primaries and replicas aren't on the same node.
- **Network considerations:** Place Redis nodes as close to the microservices as possible (same AZ for low latency). If using a load balancer (like AWS NLB) in front of Redis (not typical unless you have a custom proxy), ensure it's configured for low-latency, TCP long-lived connections. Usually, clients connect directly so LB is not needed except in special cases (e.g., if forcing TLS through a proxy).
- **Testing failover effects on app:** Similar to DB, test what happens in the app when Redis fails over. Redis Sentinel or Cluster failover can cause a few seconds of unavailability. The client might see connection resets or MOVED errors. Ensure the client library handles it (most cluster clients handle MOVED/ASK redirects). During that time, see if the app significantly degrades or if it has any retry logic for cache. Ideally, a cache miss during failover just goes to DB (with maybe a slight latency increase).
- **Plan scaling events:** For cluster mode, adding/removing shards redistributes slots - that uses CPU/network. Do it in off-peak hours and monitor. For non-cluster, scaling up means vertical scaling or manual partitioning. Plan those events and test them. Also, if using cluster, note that adding a node doesn't automatically rebalance evenly; you have to trigger a rebalance. Know your tools (e.g., redis-cli --cluster rebalance).
- **Use replication groups (AWS) or clusters to your advantage:** In AWS Elasticache, even if you don't use cluster mode, you can have up to 5 replicas in a group. This is primarily for HA, but you can also distribute read load (like heavy get traffic for the same data could be split among replicas via the Reader Endpoint, which round-robins reads). Make sure to enable Multi-AZ with automatic failover in Elasticache for production (otherwise a node outage won't auto-recover). On bare metal, ensure sentinel or cluster is configured to auto-recover without manual steps.
- **Antipatterns:**
- **Single node deployment with no replicas:** Using a lone Redis instance for critical data is a common anti-pattern when teams underestimate its importance. This gives zero redundancy - if it goes down, you lose data and service (for caches, at least service performance is impacted; for queues or primary data, it's a direct outage). Always have at least a replica or backup plan.
- **Not using clustering when scaling vertically breaks down:** Pushing one Redis box to its limits (vertical scaling) while sharding would be more effective. At some point adding more RAM or CPU hits diminishing returns or isn't possible. If you find your Redis instance is a constant bottleneck and you're on the largest machine available, the anti-pattern is refusing to shard. It's better to partition the data. This might require some application refactoring, but it's necessary for true horizontal scale.
- **Manual failover or ad-hoc fixes:** Rushing in to manually change IPs or config when Redis fails is error-prone. If you rely on manually promoting a replica by editing config files under pressure, that's an anti-pattern. It often leads to mistakes like split-brain (two masters) or extended downtime. Automate failover with tested tools.
- **All eggs in one basket - mixing different data patterns on one instance:** E.g., using one Redis for both a high-churn cache and a persistent stream of events. The cache might evict data needed by the stream or use up memory needed for queue backlog. Or one workload might cause latency spikes affecting the other. If one service needs to flush all its keys (say, flush cache on deployment), and it's the same Redis that is also the primary datastore for another service, you inadvertently wipe that data (real case if someone runs FLUSHALL on the wrong Redis). The anti-pattern is not separating workloads that have different characteristics and importance.
- **No strategy for client-side scaling issues:** If you have hundreds of microservice instances all connecting to Redis, ensure the clients aren't a bottleneck. For example, with Redis Cluster, if not using a smart client, you might go through a proxy which could be a bottleneck. Or if using Twemproxy (which doesn't support some commands), you might hit limitations. The anti-pattern is deploying at scale without evaluating if the Redis client strategy scales too. You might need to partition clients or use a tree of proxies for very large deployments.
- **Overlooking timeouts and error handling in load balanced scenarios:** If a Redis node goes down, clients connecting to it will timeout. Not setting a reasonable timeout (both connect and read/write) is anti-pattern - it can make requests hang. Always set timeouts on Redis operations. Also, in a cluster, if a shard is down, the application should not hang indefinitely for that portion of keys - handle the error (maybe respond with an error or degrade functionality related to that shard).
- **Full reliance on a single thread for pub/sub scaling:** Traditional Redis pub/sub (non-sharded) will send each published message to all subscribers, and that is done on the main thread. If you have a scenario with thousands of subscribers or very high message volume, one Redis node can become a bottleneck (CPU and network). If one channel is hot, it can delay others. Anti-pattern: trying to funnel too much through one channel or one Redis instance for pub/sub. Instead, partition channels across shards (sharded pub/sub) or use separate Redis instances for different pubsub topics, or consider a purpose-built messaging system if requirements overshoot Redis's capabilities.
- **Using a load balancer with Redis in an incompatible way:** For instance, putting a round-robin TCP load balancer in front of a Redis cluster's multiple primaries - the LB might send some keys to the wrong backend (since Redis cluster expects the client to know which node to hit). This doesn't work because the client needs to handle cluster redirection. The anti-pattern is treating Redis like an HTTP service behind a load balancer. The correct approach is to use Redis cluster client or a proxy aware of Redis protocol (which is not a generic LB).
- **Ignoring upgrades and version mismatches:** Running an old version of Redis that doesn't support your desired HA or clustering features (like using a Redis 3 without clusters in 2026 would be odd and limiting). Or having client library versions that don't fully support newer Redis server features (like client doesn't handle Redis 6 ACLs or Redis 7 pubsub enhancements). Keep components updated and compatible. An anti-pattern is to stick with an old approach (like manual sharding with key tags) when Redis has added easier solutions (like native cluster mode).

With these practices in place, PostgreSQL and Redis can be used effectively in a microservices architecture to provide robust, scalable, and maintainable data management. By avoiding the antipatterns, teams ensure that each service's database and cache integrate safely and perform well within the distributed system, whether deployed on cloud platforms like AWS or on bare metal. Adhering to these guidelines - drawn from current best practices (2024-2026) and authoritative sources - will help maintain a resilient and high-performing backend.

## Redis Use Cases: Best Practices and Antipatterns

### Redis as a Cache

**Best Practices:** When using Redis as a caching layer (to store frequently accessed data for fast retrieval):

- **Set appropriate TTLs:** Always assign an expiration to cache entries unless there's a strong reason not to. This ensures stale data eventually expires and frees memory. Determine TTL based on how fresh the data needs to be (e.g., stock prices might be seconds, user profiles could be hours). Use a jitter (random small offset) on TTLs to avoid many keys expiring simultaneously.
- **Cache only hot and expensive data:** Focus on caching data that is costly to compute or fetch, and that is requested often. Use cache metrics to identify low-value entries. It's a best practice to _not_ cache extremely infrequently accessed data (it will just consume memory needlessly) or data that changes more often than it's read.
- **Use key naming conventions:** Include identifiers in keys (e.g., user:1234:profile) to avoid collisions and make invalidation easier. A clear prefix per data type also helps (you can SCAN by prefix for admin/debug or use it to group keys for deletion when needed).
- **Choose an eviction policy suited to cache usage:** For pure cache instances, use an eviction strategy like allkeys-lru or allkeys-lfu. LRU (Least Recently Used) will evict the least recently accessed key when memory is full, which is generally good for cache. Make sure maxmemory is set to a value below physical memory to avoid OS swapping.
- **Warm the cache (when beneficial):** In some cases, after a deployment or cache flush, pre-populating the cache with certain known hot keys can reduce the thundering herd. This can be done via a background job that queries the database for popular items. Be cautious to not overwhelm the DB - rate limit warming. This is useful if you know specific keys are always needed (e.g., reference data).
- **Monitor cache hit rate:** A high hit rate (e.g., >90%) means the cache is effective. If the hit rate is low, reconsider what you cache or if the cache size is sufficient. If using AWS Elasticache, CloudWatch provides CacheHitRate. Low hit rates could mean keys churn out before being reused (maybe increase memory or TTL) or that many requests are one-time (cache might not help those).
- **Graceful degradation on cache failure:** Ensure the application can bypass the cache if needed. If Redis is down, the service should automatically fall back to direct database reads (albeit slower). It might be worth implementing a circuit breaker: if the cache is unreachable, stop trying for a short period and use DB, to avoid constant timeouts.

**Antipatterns:** Common mistakes when using Redis as a cache include:

- **No invalidation on data change:** Updating the database but not updating or invalidating the corresponding cache entry. This leads to serving stale data indefinitely (if no TTL). For example, if a user updates their profile but the old profile is still in Redis, users will see outdated info. Always invalidate or update cache on writes.
- **Caching unlimited/unbounded data sets:** Caching results of queries with unbounded result sets (e.g., "get all users") or extremely large objects. This can fill the cache and evict a lot of other useful data, or make retrieval slow (large payload). Cache should be used for relatively small, repeated data, not giant one-off results.
- **Over-caching (cache misuse):** Caching data that is almost as quick to get from the DB or that changes almost every time it's accessed. For instance, caching a counter that is incremented every second - the cache will be updated every second, providing little benefit and adding complexity. Or caching static data that rarely changes and is small, which might be fine to just keep in application memory or let the DB handle (especially if DB is not the bottleneck).
- **Assuming cache hit = up-to-date data:** Without considering race conditions or propagation delays. For example, a sequence: Service A updates DB, then immediately reads from cache (which still has old data because invalidation hasn't happened yet or has a slight delay). If not careful, you get inconsistent reads. It's an anti-pattern to not handle this; one solution is to use **cache aside**: update the DB, then delete the cache key _before_ next read. Or use versions/timestamps to validate cache freshness if needed.
- **Using the cache as the source of truth:** Storing data in Redis cache and not persisting it, assuming Redis will always have it. If it's truly disposable data, fine - but if it's important, you risk data loss on eviction or restart. For example, do not only cache user session state in Redis without persistence if losing it would log everyone out unexpectedly (unless that's acceptable). Either accept it's ephemeral or use persistent storage for truth.
- **Cache Stampede (Thundering Herd):** Not implementing protection against many clients requesting the same item that's missing. Example: a popular item expires; suddenly 100 requests all go to DB to reload it. This can crash the DB. Not handling this is an anti-pattern. Solutions were discussed (locks, request coalescing). Ignoring this can nullify the benefits of caching under heavy load.
- **Forgetting to size your cache (memory):** Assuming Redis will magically handle whatever you throw at it. If you don't set maxmemory, Redis might consume all RAM and then crash the system or start swapping (terrible for performance). If you set it but too low, useful data might evict too often. It's a mistake to not analyze memory usage patterns and adjust accordingly. Also, neglecting to consider object size overhead (Redis has per-key overhead ~50 bytes or more) - lots of small keys can add up.
- **Storing sensitive data without consideration:** Putting things like plaintext passwords, personal data, or access tokens in cache without security considerations. If Redis isn't secured or if an engineer dumps cache for debugging, that data could leak. At minimum, use TLS/auth and limit access. Also, data in cache might not be encrypted at rest (unless using encrypted filesystems or a service that does it). The anti-pattern is treating cache as outside your security model - it should be as secure as the DB if it holds sensitive info.

### Redis for Pub/Sub

**Best Practices:** Pub/Sub in Redis is used for real-time message broadcasting (one-to-many, fire-and-forget messaging):

- **Use Pub/Sub for ephemeral, real-time events:** Only use Redis Pub/Sub when it's acceptable that if a subscriber is offline, it misses messages. Good examples: notifications to live users, cache invalidation signals, real-time metrics updates. In these cases, design your system such that missing a message isn't catastrophic (e.g., a user who was offline doesn't need to get a missed chat notification because they'll fetch messages on reconnect from the DB).
- **Keep message payloads small:** Since messages are delivered to all subscribers via Redis's single thread, large messages slow down distribution. Send identifiers or concise data. If clients then need more info, they can fetch it (lazy load). For instance, publish "order 1234 shipped" (tiny), instead of the entire order JSON.
- **Monitor subscriber performance:** If a subscriber is slow to process messages, Redis will queue messages for it in memory. Identify slow consumers (Redis provides client output buffer length in CLIENT LIST). For critical systems, implement a policy: if a subscriber consistently can't keep up, maybe have it disconnected or use separate channels for high/low priority. A best practice is to **drop or ignore old messages** on slow clients if they're not catching up (which Redis effectively does by disconnecting them when output buffer limit is reached).
- **Use patterns (pSubscribe) carefully:** Pattern subscriptions (like subscribing to news.\*) are convenient. But know that a pattern subscription routes a copy of every matching message to that subscriber, potentially increasing load. If you have many pattern subscribers, consider whether you should create explicit channels instead. Use patterns where the number of subscribers is moderate and pattern matching simplifies code significantly.
- **Leverage sharded pub/sub if available:** In Redis 7+, if using cluster mode, use _sharded pubsub_ commands (or ensure your client does). This limits the scope of messages to one shard. In practice, it means instead of a global channel, you can have channel per shard or use the new commands that only go to a specific shard. For example, rather than a channel "notifications" that is cluster-wide (broadcasts to all shards and all subscribers), you could have "notifications:shard-5" that only subscribers on that shard get. Many client libraries now do this automatically when you publish to a cluster.
- **Design channel naming with scope in mind:** If not using cluster mode, you can still reduce unnecessary subscriber work by using multiple channels instead of one. E.g., if you have events for different tenant IDs, consider one channel per tenant instead of one giant "events" channel where subscribers have to sift through messages not for them. Subscribers then only subscribe where needed, reducing message traffic they ignore.
- **Graceful client reconnection:** Implement reconnection logic for subscribers. If the Redis server restarts or a network blip occurs, subscribers should reconnect and resubscribe to channels. Many Redis client libraries handle this (e.g., Node Redis client will re-subscribe after reconnect). Test this behavior. Possibly employ a small delay on reconnect attempts (exponential backoff) to avoid stampedes.

**Antipatterns:**

- **Using Pub/Sub for guaranteed delivery or critical data:** Pub/Sub provides no acknowledgement or persistence. If a subscriber is down or slow, messages are lost for it. Using Pub/Sub for, say, inter-service communication where each message _must_ be processed (like "charge customer card") is an anti-pattern. In such cases, use Redis Streams or a message queue with persistence and ack.
- **Huge numbers of subscribers on one channel:** Because Redis will iterate over every subscriber to deliver each message, thousands of subscribers on a busy channel can saturate the server. If you need that scale, consider a different design (hierarchy of channels, or a dedicated message broker like Kafka or RabbitMQ). It's an anti-pattern to treat Redis Pub/Sub as infinitely scalable for massive fan-out. It's great up to maybe hundreds of subscribers; beyond that, monitor carefully.
- **Ignoring subscriber disconnects:** If a subscriber disconnects and misses some messages, on reconnect it won't get those historical messages. If your system cannot tolerate that, it's wrong to use Pub/Sub. Don't attempt to build a history or replay mechanism on top of Redis Pub/Sub manually (like writing messages also to a list) - use Streams or another solution. The anti-pattern is trying to make Pub/Sub do what it wasn't meant to (persistent log).
- **Publishing huge messages or too many messages per second without profiling:** You might flood the network or cause high latency for all clients. If you need to send, say, a 5 MB message, that's going to block the Redis event loop while it sends to each subscriber. This could freeze other operations momentarily. Very high frequency messages (thousands per second) also could be an issue - though Redis can push a lot, it depends on subscriber count and network. Not testing these limits is an anti-pattern - always benchmark if you plan to use Pub/Sub for high volume.
- **Using Pub/Sub for cross-datacenter communication:** Pub/Sub is not designed for WAN usage or scenarios with higher latency. If you publish in one region and have subscribers in another via a mirrored Redis or something, any network hiccup can drop messages. Better to use a more robust messaging system or Streams (with replication). The anti-pattern is stretching Pub/Sub beyond a single cluster or single data center context.
- **Overlooking security on channels:** Assuming that messages are internal so no one malicious can subscribe. If Redis isn't properly secured, an attacker could subscribe to sensitive channels (or publish bogus messages). Always secure Redis as per Security section. But also, do not put extremely sensitive data in pub/sub payloads unless needed (and if so, maybe encrypt the message contents at the application level). It's an anti-pattern to trust blindly that pub/sub traffic can't be intercepted.
- **Trying to do request-response over Pub/Sub:** Pub/Sub is great for broadcast, not so much for targeted responses (though you can, e.g., publish to a unique channel for a reply). If you find yourself doing a lot of correlation (like RPC over pub/sub), consider if a direct request or a different pattern (like a Redis list for responses keyed by request) is better. Using pub/sub for something like "function call and response" can become messy and is usually an anti-pattern compared to using a queue or HTTP call with async handling.

### Redis for Queues (Background Jobs)

**Best Practices:** Redis is often used to implement lightweight queues for background job processing:

- **Use lists or streams for queue semantics:** The simplest is a Redis List acting as a queue (LPUSH for enqueue, BRPOP for dequeue). This gives a FIFO queue. For multiple consumers with reliability, combine RPOPLPUSH/BRPOPLPUSH as discussed, or use Redis Streams which have built-in support for consumer groups and ack.
- **Implement reliable processing (at-least once):** If using lists, the pattern is: BRPOP from main queue -> process item -> on success, do nothing (item is removed); if fail, optionally push item back or to a dead letter list. Alternatively, RPOPLPUSH to a processing list -> process -> LREM from processing list on success; on failure, either leave in processing (to be handled later) or move back. This ensures that if a worker dies mid-process, the item isn't lost (it's in the processing list and can be recovered). With Streams, use XREADGROUP to get an entry and XACK after processing; use XPENDING to see unacked messages and XCLAIM to retry if needed.
- **Monitor queue length and age:** Track the length of the queue (LLEN for list, XLEN for stream) and how long items are waiting (perhaps by including a timestamp in the job payload). Alert if the queue grows beyond normal or if jobs are stuck for too long - this indicates consumers can't keep up or have failed. This helps maintain system health and throughput.
- **Use a Dead Letter Queue (DLQ):** Have a mechanism for jobs that keep failing. For example, after X retries, move the job to a "dead" queue or stream for inspection, instead of endlessly retrying. This prevents poison messages from clogging the queue.
- **Keep job payloads small and self-contained:** The job message should have all info to process or an ID to fetch more info. Don't put huge blobs in the queue. If workers need a lot of data (like an image), it might be better to store the image in S3 and have the job contain the reference.
- **Consider using existing libraries:** There are many libraries (Bull for Node.js, Sidekiq for Ruby uses Redis, etc.) that implement robust job queues with Redis. They handle a lot of these patterns (scheduling, retries, etc.). Use them instead of reinventing the wheel, to avoid subtle mistakes.
- **Use atomic operations for ack/nack:** If not using streams (which have XACK), ensure your remove or requeue is atomic with respect to processing. The RPOPLPUSH pattern is atomic for moving to processing. When removing from processing after success, use LREM with a count of 1 to remove the specific job (make sure the job ID is unique or content unique enough to find it).
- **Parallelism and fairness:** If you have multiple types of jobs, you might use multiple queues (one per type) so that a flood of one type doesn't starve others. Workers can then poll multiple queues (or use BLPOP with multiple keys which returns from whichever queue gets an item first). This gives a simple prioritization if you order the keys (BLPOP checks list order).
- **Back-pressure on producers:** If queue gets too long, consider slowing down producers or rejecting tasks. Otherwise memory can be exhausted. You can implement a check: if LLEN exceeds N, producers get told to wait or drop less important tasks. This prevents infinite buildup.
- **Ensure idempotent or transactional processing:** Since a job may be retried, design the processing to be idempotent (safe to do twice). E.g., if a job says "send email to X", tag the email with an identifier and on processing check if already sent to avoid dupes. Or use a transactional outbox pattern if the job corresponds to a DB change, etc. Idempotency greatly simplifies error handling.

**Antipatterns:**

- **Using Redis queues with no retry or tracking:** Simply RPOP a job and if the worker dies, that job is gone forever (lost). This is an anti-pattern because it sacrifices reliability. It's surprisingly common in simplistic setups. Always design with failure in mind (the reliable queue pattern or streams).
- **Infinite retry loops or blocking issues:** If a job fails and immediately requeues itself unconditionally, it could create a retry storm (especially if the failure is due to a persistent issue). That can overwhelm the system. Implement exponential backoff for retries or a max retry count. Similarly, a worker that doesn't use BRPOP and instead polls in a tight loop when queue is empty is wasteful - use blocking or sleep between polls.
- **No consumer health check:** If all consumers die but producers keep adding, the queue will fill Redis memory (if no bound) or tasks will just sit indefinitely. It's bad practice to not have an alert on a stagnant queue (items enqueued but not dequeued). You should notice if processing has stopped.
- **Very large jobs or long-running jobs in Redis queue:** If a job takes hours to process, leaving it in a Redis queue might not be ideal - if it fails at hour 2, you might want a more robust system that can resume or handle such long tasks. Redis is typically for quick jobs. Long processing might also mean your worker holds a job and isn't acking for a long time, which could be seen as "stuck." For such cases, a more heavyweight message broker or breaking the task into smaller sub-tasks is wiser.
- **Using Redis for strictly ordered, persistent event logs:** If you need a guarantee of processing every item in order, and to replay them, Redis lists/streams start to mimic a log like Kafka. While Redis Streams provide some features, if the requirement is heavy (very high throughput, strict ordering across many consumers, disk persistence), using Kafka or another purpose-built queue might be better. An anti-pattern would be to push Redis beyond its sweet spot for queues - e.g., millions of pending messages and treating it as a permanent store (streams will retain data until trimmed, but if you never trim, memory/disk usage can blow up).
- **Not handling failures in workers:** If a worker pops a job, tries to process but the process crashes (perhaps the whole container), that job is lost (in a simple list scenario). If one isn't using a safe pattern, that's data loss. Failing to address this (just hoping workers never crash mid-task) is an anti-pattern.
- **Storing queue in one giant Redis key (like a big list or sorted set) with expensive operations:** E.g., using a Sorted Set as a priority queue is okay, but operations like ZPOPmin are O(log N). If N gets large (hundreds of thousands or millions), each op might start to lag. If extremely large, consider segmenting or at least monitoring the cost. The anti-pattern is not understanding the scaling complexity - e.g., doing a KEYS \* to check queue content size (very bad).
- **Processing directly in Redis with EVAL (rare):** It would be an anti-pattern to try to do the whole job inside Redis via a Lua script (except trivial stuff) - that blocks Redis completely for other clients. Always offload real processing to worker processes outside Redis.

### Redis Streams

**Best Practices:** Redis Streams (available since Redis 5) provide a log-like data structure good for ordered events and consumer groups:

- **Use consumer groups for scaling consumers:** A consumer group allows multiple consumers to share the stream, each getting a subset of messages. This is great for parallel processing. Create a consumer group with a name, and each worker uses a unique consumer name within that group. Redis will distribute messages to consumers, and you can add consumers as needed.
- **Persist and replay events as needed:** Streams persist data until you trim it. If you need durability of events (similar to Kafka-lite), set an appropriate retention policy (e.g., trim after X days or when stream length > Y). This allows new consumers or offline consumers to catch up on missed events. If persistence is crucial, also ensure AOF persistence is on, so stream entries are not lost on crash (streams are part of the dataset).
- **Acknowledge and track processing:** After processing an entry, call XACK to acknowledge it so it can be removed from the Pending Entries List (PEL) for that consumer group. Regularly monitor XPENDING to see if there are messages stuck unacknowledged (maybe a consumer died). You can automate idle message detection by XAUTOCLAIM (Redis 6.2+) which can claim messages that have been idle (unacked) for too long to another consumer. This ensures no message is left forever if a consumer went away.
- **Trim the stream (when appropriate):** If you don't need infinite history, use XTRIM to cap the length of the stream (or approximate length with ~ option for performance). For example, keep last 1 million events or last 7 days of events. This prevents uncontrolled memory growth. Do this carefully if consumers might not yet process trimmed entries - typically trim only entries already acknowledged by all groups or use XTRIM with MINID to trim safely. In use-cases like event sourcing where infinite history is needed, offload older data to storage (or be prepared to scale memory/disk).
- **Use stream IDs and ranges for efficient querying:** If you need to fetch a range of events, use XRANGE or XREAD with IDs. Design IDs (they are usually time-based, but you can attach an sequence) to easily pick up where you left off. Usually, you use the last seen ID as the start for the next read (this is what XREADGROUP does automatically with > as ID to get new messages after last delivered).
- **Ensure idempotency in consumers:** With streams, consumers might get the same message twice (e.g., if a message is not acknowledged before a claim by another consumer, two might process it or on restart if you didn't ack it might re-deliver). Thus, like with queues, handle duplicates or make operations idempotent.
- **Leverage streams for multi-subscriber scenarios:** If you have a case where multiple different services need to consume the same event (like an event bus pattern), streams are suitable because you can have multiple consumer groups on the same stream, each representing a different service or processing pipeline. Each group gets its own copy of the data (unlike a queue which usually one consumer reads and it's gone). This is a big advantage over pub/sub because if one service is down, it can catch up later from the stream.
- **Memory management for streams:** Understand that streams use memory not just for the data but also for metadata in PELs. Deleting an entry that hasn't been acknowledged by all consumer groups won't actually free it (it's still in PEL). So, ensure consumer groups ack and you trim data that's truly done. Monitor the stream's length and PEL sizes (XPENDING can show how many pending per group). If a group is no longer needed, delete it (XGROUP DESTROY) so its pending references are cleared.
- **Use MAXLEN with ~ for trimming to avoid blocking:** If you set up XADD with MAXLEN (approximate trimming), Redis will trim in O(n) for n trimmed, but using ~ makes it faster by not trimming every single add (just when overhead becomes significant). This trades perfect precision for performance. Best practice is to use XADD ... MAXLEN ~ 10000 rather than without ~ if you want to auto-trim.

**Antipatterns:**

- **Treating Streams like Pub/Sub for huge fan-out without need:** If you have only one consumer type and never need replay, using a stream (with its overhead) instead of Pub/Sub can be overkill. Streams shine when you need durability or multiple consumer groups. If not, pub/sub might suffice. An anti-pattern is adding complexity of streams when you don't need those features.
- **Infinite untrimmed streams with no archival:** Letting a stream grow forever in memory is an anti-pattern unless you plan for it (and have infinite memory, which you don't). If you need an infinite log, consider if Redis is the right storage (maybe a disk-based log like Kafka or RediSearch JSON might be better). At least offload old data periodically. Don't just forget about old events.
- **Not handling consumer failures (PEL buildup):** If you never check XPENDING or have no strategy for stuck messages, you could have a backlog in PEL that never gets processed (e.g., if a consumer crashed after getting some messages, those are pending forever until XCLAIM). This is an anti-pattern: you're "losing" messages because you never acknowledge or reassign them. Always have a plan for monitoring and recovering pending messages (set an idle timeout to detect).
- **Using streams for very high throughput with large messages and many groups:** While Redis streams are powerful, they still operate on a single node (unless you shard by using multiple streams). A design that requires, say, 100k messages/sec with 10 consumer groups and messages of 1KB each might push the limits. The anti-pattern is not testing this scenario or not partitioning the data. If needed, partition across multiple streams (like by some key) so that not all consumers handle all data if it's not needed.
- **Storing extremely large messages or blobs in streams:** Similar to other cases, it's not efficient. The stream log will become huge and slow to traverse. Better to store large payloads elsewhere and reference them.
- **Using blocking XREAD in a tight loop incorrectly:** If you use XREAD (for new messages) in a loop, ensure you use the blocking mode with a timeout, or you might accidentally hammer Redis. The anti-pattern would be to poll with XREAD with no block thousands of times per second. Instead, use BLOCK with a reasonable timeout.
- **Not distinguishing different event types:** If you put completely different types of events into one stream (e.g., user events, system metrics, logs all in one), you might complicate consumer logic and retention policies. It could be better to have separate streams for separate concerns. Overloading one stream can be an anti-pattern if it leads to inefficiency (consumers skipping irrelevant messages, etc.).
- **Misusing stream IDs:** The stream ID has a timestamp part. An anti-pattern is to manually assign IDs out-of-order or with old timestamps - that can reorder events. Always append with \* to let Redis assign, unless you have a specific reason and you understand the consequences. Also, do not use very far-future timestamps as IDs - it can block adding new entries with current time until that time passes (since stream IDs must increase).

### Redis as a Primary Key-Value Data Store

**Best Practices:** Some architectures use Redis as their primary database for certain data (for speed at the cost of complex querying):

- **Enable AOF or RDB for persistence:** If Redis is the source of truth for data (meaning data cannot be lost), turn on persistence. Typically AOF (Append Only File) with appendfsync everysec is a good balance (at most 1 second of data loss in a crash). You can also take periodic RDB snapshots as backups. In managed services, just ensure the snapshot schedule is on.
- **Use replication and failover:** Have replicas and a failover plan (Sentinel or cluster). Essentially treat Redis like you would a database - multiple replicas, possibly even cross-data-center replication if needed for DR (though Redis async replication can have data loss on failover - consider wait for replication (WAIT command) if needed to ensure replication to at least one replica).
- **Set memory size appropriately and avoid eviction for primary data:** Ideally, if Redis is your DB, you should not run it with an eviction policy that deletes data (noeviction or volatile-\* only if you truly mark some keys as volatile). Plan capacity such that all data fits in memory with headroom. If data size is larger than what one machine's RAM can hold, consider Redis Enterprise or Redis-on-Flash, or sharding data across multiple Redis instances (like a cluster) so aggregate memory is enough.
- **Data modeling for efficient access:** Model your data with Redis data structures to get the best performance. If you need to look up by different fields, maybe maintain secondary indexes in sets or sorted sets (pointing from field value to keys). This is manual but can be done. For example, store users in a hash by ID, and also have a sorted set of user IDs by creation date if you need to query that. This is analogous to maintaining indexes in a relational DB, but you do it at app level. It's a best practice to keep such indexes updated in the same operation (MULTI/EXEC transaction) as writing the main data to avoid inconsistency.
- **Employ modules if suitable:** Redis modules like RedisJSON, RediSearch can augment Redis to be more like a document store or provide query capabilities. If using Redis as primary store for complex data, these modules can be helpful (e.g., RedisJSON allows you to store JSON documents and query/edit parts of them; RediSearch gives you secondary index querying on hashes/JSON). They come with some memory overhead but can make Redis a more complete primary store solution.
- **Backup data periodically:** Even with AOF, backup the AOF or RDB files to an external storage regularly. If the dataset is not huge, you can do RDB snapshots and ship them off. In cloud, enable snapshot to S3. This is important in case of catastrophic failures (node loss with disk loss, or logical errors that delete data - you want an external backup to restore from).
- **Plan for data migration or growth:** If data grows, you might need to migrate to a cluster or add memory. Have a plan to reshuffle data with minimal downtime (for example, if initially on one node and need to move to cluster, you might spin up a cluster and dual-write during migration or use Redis replication to seed data into cluster shards one by one). It's best practice to factor this possibility early if you foresee growth (maybe start with cluster mode even if one shard, so adding shards later is easier).
- **Use appropriate data types to save space:** For instance, if you store a lot of integers, consider using bitmaps or HyperLogLog for certain set membership/unique counts to save space. Use hashes to store multiple small fields under one key rather than many keys (Redis hashes are memory-efficient for small field-values; many small keys have more overhead). E.g., instead of 100 keys for 100 user attributes, one hash key with 100 fields. This reduces key count and memory overhead.
- **Implement application-level logic for multi-key operations:** Redis doesn't enforce relational integrity or multi-key transactions (unless using MULTI which is still on one server). If you need to update two related keys, wrap it in a MULTI/EXEC to ensure atomicity. E.g., if deducting from one account and adding to another (in a banking scenario) - both in Redis - do it in one transaction with checks (or use Lua script to ensure atomic). Basically, mimic database transactions where needed, since Redis doesn't have them across shards or without explicit MULTI.

**Antipatterns:**

- **Using Redis as a primary store without persistence (volatile)**: This is essentially like an in-memory database with no safety net. If Redis restarts for any reason (crash, power cycle), all data is gone. Unless the data is truly ephemeral or rebuildable, this is a huge risk. It's an anti-pattern to run a critical dataset purely in-memory without at least snapshotting or AOF. Sadly, some do this for speed and regret it after an outage.
- **Relying on Redis for complex querying of primary data:** If you find yourself needing to query by value, range, or do aggregations on data in Redis, you might be forcing a use-case it's not designed for. For example, scanning through all user hashes to find users older than 30 - doing that in Redis means a SCAN of entire dataset (which is O(N)). This will be slow at scale and block other operations. That's an anti-pattern. Instead, maintain a secondary index (like a Sorted Set keyed by age, or use RediSearch module to query). If such patterns grow, maybe a traditional database or search engine fits better.
- **Ignoring memory overhead and fragmentation:** When Redis is your primary DB, you must pay attention to memory use. Over time, deleting and adding keys can fragment memory (on Redis < 6 without active defrag or if defrag can't keep up). This can lead to higher RSS memory than actual data size. It's an anti-pattern to never monitor or defragment (Redis has an MEMORY DOCTOR command that gives advice). Also, complex structures like big sorted sets or large lists might consume more memory than expected due to allocations. Failing to account for this may cause out-of-memory sooner than anticipated.
- **All data on one node without sharding when beyond capacity:** If your data grows beyond RAM of a single machine and you start evicting valuable data (but it's primary, you can't really evict), that's an anti-pattern. Some might try to use volatile-lru on a primary dataset with TTLs to shed load, but if it's primary, losing data is bad. The proper step is to shard or scale vertically to a bigger instance (to a point). Not re-architecting when a single node can't handle it is a mistake - you might experience crashes or massive performance drop from constant swapping/eviction.
- **Neglecting durability requirements:** Perhaps you assume AOF every second is fine, but in reality even 1 second of data loss could be a big deal for your application (financial transactions, for example). If so, the anti-pattern is not using appendfsync=always or synchronous replication. There's a trade-off: always will slow down writes significantly (as it waits for disk every write). If that's unacceptable, maybe Redis isn't the right primary store for that piece of data where no loss can be tolerated. Or you layer on another system for audit. But ignoring this requirement is dangerous.
- **Storing large blobs or binary files in Redis:** Some might be tempted to store images or large JSON documents in Redis because it's the primary store. That's an anti-pattern; it will balloon memory. Better to store such blobs in an object store or as a file, and keep references in Redis. Use Redis for what needs ultra-low latency and is accessed frequently; huge blobs are better in a CDN or database/file system.
- **No schema or structure management:** Just because Redis is schema-less (key-value) doesn't mean one should dump data in random structures without documentation. In a team, you should have an agreed schema for keys and values (like which hashes have which fields, which sets contain what). Without this, it becomes a mess and hard to maintain or migrate. Not having a clear data model is an anti-pattern leading to inconsistent usage and bugs.
- **Trying to achieve cross-shard transactions or relations in cluster:** If you require operations that affect multiple keys on different shards (in Redis Cluster), it's not possible to do atomically (Lua scripts or MULTI won't work across shards). Designing a system needing that and still using Redis Cluster is an anti-pattern. Either constrain data that needs atomic ops to the same shard (via key hashing tags) or don't use Redis for that part. For example, don't try to implement a multi-key bank transfer if the two accounts hash to different shards - you can't do it safely if it's truly inter-dependent (except using a poor-man's 2PC in app code, which is complex and not guaranteed).
- **Running without proper backups or disaster recovery:** With a primary DB, you must consider worst-case: entire Redis cluster down, or data corrupted by a bad deployment (imagine a script accidentally FLUSHALL). If you have no backups, that's irrecoverable. It's an anti-pattern to forgo backups because "Redis is in-memory and fast" - you can dump memory to disk (RDB) and back it up, it's worth the time for critical data. Also test restore occasionally.
- **Overlooking the cost of scale:** Using Redis as primary may require large memory nodes or many shards, which can be expensive. Sometimes people choose Redis for speed but ignore costs - e.g., 1TB of RAM across cluster vs maybe that data could reside mostly on disk in a SQL DB with caching. It's not a technical anti-pattern per se, but architecturally, reconsider if only a small hot subset needs to be in Redis and cold data in a cheaper store. All-in on Redis for everything can be overkill and costly.

